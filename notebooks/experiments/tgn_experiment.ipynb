{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Temporal Graph Network Experiment (EvolveGCN-style)\n",
        "\n",
        "This notebook mirrors the experimental protocol used for the EvolveGCN runs: multiple observation windows, multiple random seeds, per-cohort training/evaluation, and detailed metric/prediction logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "while PROJECT_ROOT != PROJECT_ROOT.parent and not (PROJECT_ROOT / \"code_lib\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "if not (PROJECT_ROOT / \"code_lib\").exists():\n",
        "    raise RuntimeError(\"Unable to locate 'code_lib' directory from current working directory\")\n",
        "\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from code_lib.temporal_node_classification_builder import (\n",
        "    TemporalNodeClassificationBuilder,\n",
        "    load_elliptic_data,\n",
        "    prepare_temporal_model_graphs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from test_config import EXPERIMENT_CONFIG\n",
        "\n",
        "CONFIG = EXPERIMENT_CONFIG.copy()\n",
        "CONFIG['device'] = CONFIG.get('device', 'cuda:0')\n",
        "\n",
        "SEEDS = [42, 123, 456]\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\" / \"tgn_multi_seed\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "EPOCHS = 350\n",
        "PATIENCE = 50\n",
        "LAM_GAP = 0.2\n",
        "COHORT_BSZ = 1\n",
        "USE_SCHED = True\n",
        "\n",
        "device_str = CONFIG['device']\n",
        "if device_str.startswith(\"cuda\") and not torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "else:\n",
        "    DEVICE = torch.device(device_str)\n",
        "\n",
        "print(\"Experiment settings:\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Seeds: {SEEDS}\")\n",
        "print(f\"  Observation windows: {CONFIG['observation_windows']}\")\n",
        "print(f\"  Results directory: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = PROJECT_ROOT / \"elliptic_dataset\"\n",
        "nodes_df, edges_df = load_elliptic_data(str(DATA_DIR), use_temporal_features=True)\n",
        "\n",
        "builder = TemporalNodeClassificationBuilder(\n",
        "    nodes_df=nodes_df,\n",
        "    edges_df=edges_df,\n",
        "    include_class_as_feature=False,\n",
        "    add_temporal_features=True,\n",
        "    add_edge_weights=False,\n",
        "    cache_dir=str(PROJECT_ROOT / \"graph_cache_tgn\"),\n",
        "    use_cache=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "split = builder.get_train_val_test_split(\n",
        "    train_timesteps=CONFIG['train_timesteps'],\n",
        "    val_timesteps=CONFIG['val_timesteps'],\n",
        "    test_timesteps=CONFIG['test_timesteps'],\n",
        "    filter_unknown=True,\n",
        ")\n",
        "\n",
        "print(f\"Train nodes: {len(split['train'])}\")\n",
        "print(f\"Val nodes:   {len(split['val'])}\")\n",
        "print(f\"Test nodes:  {len(split['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequences = prepare_temporal_model_graphs(\n",
        "    builder,\n",
        "    split['train'],\n",
        "    split['val'],\n",
        "    split['test'],\n",
        "    K_values=CONFIG['observation_windows'],\n",
        "    device=torch.device('cpu'),\n",
        ")\n",
        "\n",
        "address_to_idx = {addr: idx for idx, addr in enumerate(builder.all_addresses)}\n",
        "num_nodes = len(address_to_idx)\n",
        "\n",
        "def find_sample_graph(seq_dict):\n",
        "    for split_dict in seq_dict.values():\n",
        "        for split_name in ['train', 'val', 'test']:\n",
        "            cohorts = split_dict.get(split_name, {}).get('cohorts', [])\n",
        "            for cohort in cohorts:\n",
        "                if cohort['graphs']:\n",
        "                    return cohort['graphs'][0]\n",
        "    raise RuntimeError('Unable to locate a sample graph to infer feature dimensions.')\n",
        "\n",
        "sample_graph = find_sample_graph(sequences)\n",
        "feature_dim = sample_graph.x.shape[1]\n",
        "raw_msg_dim = feature_dim * 2\n",
        "\n",
        "print(f\"Total nodes tracked: {num_nodes}\")\n",
        "for K in CONFIG['observation_windows']:\n",
        "    seq = sequences.get(K, {})\n",
        "    train_count = len(seq.get('train', {}).get('cohorts', []))\n",
        "    val_count = len(seq.get('val', {}).get('cohorts', []))\n",
        "    test_count = len(seq.get('test', {}).get('cohorts', []))\n",
        "    print(f\"K={K}: train cohorts={train_count}, val cohorts={val_count}, test cohorts={test_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def local_to_global_tensor(graph):\n",
        "    if not hasattr(graph, 'node_address'):\n",
        "        raise RuntimeError('Graph is missing node_address metadata. Rebuild with return_node_metadata=True.')\n",
        "    idxs = [address_to_idx[addr] for addr in graph.node_address]\n",
        "    return torch.tensor(idxs, dtype=torch.long, device=DEVICE)\n",
        "\n",
        "def ingest_graph(memory, graph):\n",
        "    if graph.edge_index.numel() == 0:\n",
        "        return\n",
        "    local_to_global = local_to_global_tensor(graph)\n",
        "    edge_index = graph.edge_index\n",
        "    src_idx = edge_index[0]\n",
        "    dst_idx = edge_index[1]\n",
        "    src = local_to_global[src_idx].to(DEVICE)\n",
        "    dst = local_to_global[dst_idx].to(DEVICE)\n",
        "    t_tensor = torch.full((src.size(0),), int(graph.timestep), dtype=torch.long, device=DEVICE)\n",
        "    src_feat = graph.x[src_idx].to(DEVICE)\n",
        "    dst_feat = graph.x[dst_idx].to(DEVICE)\n",
        "    msg = torch.cat([src_feat, dst_feat], dim=-1)\n",
        "    memory.update_state(src, dst, t_tensor, msg)\n",
        "\n",
        "def classify_nodes(memory, decoder, final_graph, eval_indices):\n",
        "    if eval_indices.numel() == 0:\n",
        "        return None, None\n",
        "    local_to_global = local_to_global_tensor(final_graph)\n",
        "    eval_idx = eval_indices.to(torch.long)\n",
        "    eval_global = local_to_global[eval_idx.to(DEVICE)]\n",
        "    if eval_global.numel() == 0:\n",
        "        return None, None\n",
        "    embeddings, _ = memory(eval_global)\n",
        "    logits = decoder(embeddings)\n",
        "    labels = final_graph.y[eval_idx].to(DEVICE)\n",
        "    labels = (labels == 1).long()\n",
        "    return logits, labels\n",
        "\n",
        "def compute_class_weights(cohorts):\n",
        "    all_labels = []\n",
        "    for cohort in cohorts:\n",
        "        if not cohort['graphs']:\n",
        "            continue\n",
        "        final_graph = cohort['graphs'][-1]\n",
        "        eval_indices = cohort['eval_indices']\n",
        "        if eval_indices.numel() == 0:\n",
        "            continue\n",
        "        labels = final_graph.y[eval_indices].cpu()\n",
        "        labels = (labels == 1).long()\n",
        "        all_labels.append(labels)\n",
        "    if not all_labels:\n",
        "        return torch.tensor([1.0, 1.0], device=DEVICE)\n",
        "    labels = torch.cat(all_labels)\n",
        "    counts = torch.bincount(labels, minlength=2).float() + 1e-6\n",
        "    weights = torch.sqrt(1.0 / counts)\n",
        "    weights = weights / weights.sum() * 2.0\n",
        "    return weights.to(DEVICE)\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', pos_label=1, zero_division=0\n",
        "    )\n",
        "    if len(np.unique(y_true)) > 1:\n",
        "        auc = roc_auc_score(y_true, y_prob)\n",
        "    else:\n",
        "        auc = 0.5\n",
        "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}\n",
        "\n",
        "def evaluate_tgn(memory, decoder, cohorts):\n",
        "    memory.eval()\n",
        "    decoder.eval()\n",
        "    labels_all, preds_all, probs_all = [], [], []\n",
        "    for cohort in cohorts:\n",
        "        if not cohort['graphs']:\n",
        "            continue\n",
        "        memory.reset_state()\n",
        "        with torch.no_grad():\n",
        "            for graph in cohort['graphs']:\n",
        "                ingest_graph(memory, graph)\n",
        "            logits, labels = classify_nodes(memory, decoder, cohort['graphs'][-1], cohort['eval_indices'])\n",
        "            if logits is None:\n",
        "                continue\n",
        "            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "            preds = logits.argmax(dim=1).cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "            probs_all.append(probs)\n",
        "            preds_all.append(preds)\n",
        "            labels_all.append(labels_np)\n",
        "    if not labels_all:\n",
        "        return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.5}\n",
        "    y_true = np.concatenate(labels_all)\n",
        "    y_pred = np.concatenate(preds_all)\n",
        "    y_prob = np.concatenate(probs_all)\n",
        "    return compute_metrics(y_true, y_pred, y_prob)\n",
        "\n",
        "def collect_predictions(memory, decoder, cohorts):\n",
        "    memory.eval()\n",
        "    decoder.eval()\n",
        "    outputs = {\n",
        "        'node_indices': [],\n",
        "        'predictions': [],\n",
        "        'probs_class_0': [],\n",
        "        'probs_class_1': [],\n",
        "        'true_labels': [],\n",
        "        'timesteps': [],\n",
        "    }\n",
        "    for cohort in cohorts:\n",
        "        if not cohort['graphs']:\n",
        "            continue\n",
        "        memory.reset_state()\n",
        "        with torch.no_grad():\n",
        "            for graph in cohort['graphs']:\n",
        "                ingest_graph(memory, graph)\n",
        "            final_graph = cohort['graphs'][-1]\n",
        "            logits, labels = classify_nodes(memory, decoder, final_graph, cohort['eval_indices'])\n",
        "            if logits is None:\n",
        "                continue\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
        "            outputs['node_indices'].append(local_to_global_tensor(final_graph)[cohort['eval_indices']].cpu().numpy())\n",
        "            outputs['predictions'].append(np.argmax(probs, axis=1))\n",
        "            outputs['probs_class_0'].append(probs[:, 0])\n",
        "            outputs['probs_class_1'].append(probs[:, 1])\n",
        "            true = labels.cpu().numpy()\n",
        "            outputs['true_labels'].append(true)\n",
        "            outputs['timesteps'].append(\n",
        "                np.full_like(true, fill_value=int(final_graph.timestep), dtype=np.int64)\n",
        "            )\n",
        "    for key, value in outputs.items():\n",
        "        if value:\n",
        "            outputs[key] = np.concatenate(value)\n",
        "        else:\n",
        "            outputs[key] = np.array([])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    memory_dim: int\n",
        "    time_dim: int\n",
        "    msg_hidden: int\n",
        "    msg_out: int\n",
        "    decoder_hidden: int\n",
        "    dropout: float\n",
        "    lr: float\n",
        "    weight_decay: float\n",
        "\n",
        "model_cfg = ModelConfig(\n",
        "    memory_dim=64,\n",
        "    time_dim=8,\n",
        "    msg_hidden=64,\n",
        "    msg_out=32,\n",
        "    decoder_hidden=64,\n",
        "    dropout=0.3,\n",
        "    lr=2e-3,\n",
        "    weight_decay=1e-5,\n",
        ")\n",
        "\n",
        "class MLPMessage(nn.Module):\n",
        "    def __init__(self, raw_msg_dim, hidden_dim, out_dim, dropout, memory_dim, time_dim):\n",
        "        super().__init__()\n",
        "        in_dim = 2 * memory_dim + raw_msg_dim + time_dim\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "        )\n",
        "        self.out_channels = out_dim\n",
        "\n",
        "    def forward(self, z_src, z_dst, raw_msg, t_enc):\n",
        "        h = torch.cat([z_src, z_dst, raw_msg, t_enc], dim=-1)\n",
        "        return self.net(h)\n",
        "\n",
        "class TGNNodeClassifier(nn.Module):\n",
        "    def __init__(self, memory_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(memory_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        return self.net(embeddings)\n",
        "\n",
        "from torch_geometric.nn.models.tgn import TGNMemory, MeanAggregator\n",
        "\n",
        "def build_tgn_components(cfg):\n",
        "    message_module = MLPMessage(\n",
        "        raw_msg_dim=raw_msg_dim,\n",
        "        hidden_dim=cfg.msg_hidden,\n",
        "        out_dim=cfg.msg_out,\n",
        "        dropout=cfg.dropout,\n",
        "        memory_dim=cfg.memory_dim,\n",
        "        time_dim=cfg.time_dim,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    memory = TGNMemory(\n",
        "        num_nodes=num_nodes,\n",
        "        raw_msg_dim=raw_msg_dim,\n",
        "        memory_dim=cfg.memory_dim,\n",
        "        time_dim=cfg.time_dim,\n",
        "        message_module=message_module,\n",
        "        aggregator_module=MeanAggregator(),\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    decoder = TGNNodeClassifier(\n",
        "        memory_dim=cfg.memory_dim,\n",
        "        hidden_dim=cfg.decoder_hidden,\n",
        "        dropout=cfg.dropout,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        list(memory.parameters()) + list(decoder.parameters()),\n",
        "        lr=cfg.lr,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=2, threshold=1e-4, min_lr=1e-5, verbose=False\n",
        "    ) if USE_SCHED else None\n",
        "\n",
        "    return memory, decoder, optimizer, scheduler\n",
        "\n",
        "def train_epoch_tgn(memory, decoder, optimizer, cohorts, criterion):\n",
        "    memory.train()\n",
        "    decoder.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for cohort in cohorts:\n",
        "        if not cohort['graphs']:\n",
        "            continue\n",
        "        memory.reset_state()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for graph in cohort['graphs']:\n",
        "            ingest_graph(memory, graph)\n",
        "\n",
        "        logits, labels = classify_nodes(memory, decoder, cohort['graphs'][-1], cohort['eval_indices'])\n",
        "        if logits is None:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(list(memory.parameters()) + list(decoder.parameters()), 1.0)\n",
        "        optimizer.step()\n",
        "        memory.detach()\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / max(total_samples, 1)\n",
        "    acc = total_correct / max(total_samples, 1) if total_samples else 0.0\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_seeds_results = {}\n",
        "all_seeds_predictions = {}\n",
        "history_records = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"{'#' * 80}# SEED {seed}{'#' * 80}\")\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    seed_results = {}\n",
        "    seed_predictions = {}\n",
        "\n",
        "    for K in CONFIG['observation_windows']:\n",
        "        seq_data = sequences.get(K)\n",
        "        if seq_data is None:\n",
        "            print(f\"K={K}: no sequences available, skipping.\")\n",
        "            continue\n",
        "\n",
        "        train_cohorts = seq_data['train']['cohorts']\n",
        "        val_cohorts = seq_data['val']['cohorts']\n",
        "        test_cohorts = seq_data['test']['cohorts']\n",
        "\n",
        "        if not train_cohorts:\n",
        "            print(f\"K={K}: no training cohorts, skipping.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"K={K} | train cohorts: {len(train_cohorts)} | val cohorts: {len(val_cohorts)} | test cohorts: {len(test_cohorts)}\")\n",
        "\n",
        "        class_weights = compute_class_weights(train_cohorts)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "        memory, decoder, optimizer, scheduler = build_tgn_components(model_cfg)\n",
        "\n",
        "        best_selector = -float('inf')\n",
        "        best_state = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        pbar = tqdm(range(EPOCHS), desc=f\"Seed={seed}, K={K}\")\n",
        "        for epoch in pbar:\n",
        "            train_loss, train_acc = train_epoch_tgn(memory, decoder, optimizer, train_cohorts, criterion)\n",
        "\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                train_metrics = evaluate_tgn(memory, decoder, train_cohorts)\n",
        "                val_metrics = evaluate_tgn(memory, decoder, val_cohorts)\n",
        "                gap = abs(train_metrics['auc'] - val_metrics['auc'])\n",
        "                selector = val_metrics['auc'] - LAM_GAP * gap\n",
        "\n",
        "                if scheduler is not None:\n",
        "                    scheduler.step(val_metrics['auc'])\n",
        "\n",
        "                history_records.append({\n",
        "                    'seed': seed,\n",
        "                    'K': K,\n",
        "                    'epoch': epoch + 1,\n",
        "                    'train_loss': train_loss,\n",
        "                    'train_auc': train_metrics['auc'],\n",
        "                    'train_f1': train_metrics['f1'],\n",
        "                    'val_auc': val_metrics['auc'],\n",
        "                    'val_f1': val_metrics['f1'],\n",
        "                    'selector': selector,\n",
        "                    'gap': gap,\n",
        "                })\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{train_loss:.4f}\",\n",
        "                    'val_auc': f\"{val_metrics['auc']:.4f}\",\n",
        "                    'val_f1': f\"{val_metrics['f1']:.4f}\",\n",
        "                    'sel': f\"{selector:.4f}\",\n",
        "                })\n",
        "\n",
        "                if selector > best_selector:\n",
        "                    best_selector = selector\n",
        "                    patience_counter = 0\n",
        "                    best_state = {\n",
        "                        'memory': copy.deepcopy(memory.state_dict()),\n",
        "                        'decoder': copy.deepcopy(decoder.state_dict()),\n",
        "                    }\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "\n",
        "                if patience_counter >= PATIENCE:\n",
        "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                    break\n",
        "\n",
        "        if best_state is not None:\n",
        "            memory.load_state_dict(best_state['memory'])\n",
        "            decoder.load_state_dict(best_state['decoder'])\n",
        "\n",
        "        train_metrics = evaluate_tgn(memory, decoder, train_cohorts)\n",
        "        val_metrics = evaluate_tgn(memory, decoder, val_cohorts)\n",
        "        test_metrics = evaluate_tgn(memory, decoder, test_cohorts)\n",
        "\n",
        "        print(f\"Train F1={train_metrics['f1']:.4f} | Val F1={val_metrics['f1']:.4f} | Test F1={test_metrics['f1']:.4f}\")\n",
        "\n",
        "        seed_results[K] = {\n",
        "            'train': train_metrics,\n",
        "            'val': val_metrics,\n",
        "            'test': test_metrics,\n",
        "        }\n",
        "\n",
        "        for split_name, cohorts in [('train', train_cohorts), ('val', val_cohorts), ('test', test_cohorts)]:\n",
        "            preds = collect_predictions(memory, decoder, cohorts)\n",
        "            seed_predictions.setdefault(K, {})[split_name] = preds\n",
        "            save_path = RESULTS_DIR / f\"seed{seed}_k{K}_{split_name}_predictions.npz\"\n",
        "            np.savez_compressed(\n",
        "                save_path,\n",
        "                node_indices=preds['node_indices'],\n",
        "                predictions=preds['predictions'],\n",
        "                probs_class_0=preds['probs_class_0'],\n",
        "                probs_class_1=preds['probs_class_1'],\n",
        "                true_labels=preds['true_labels'],\n",
        "                timesteps=preds['timesteps'],\n",
        "            )\n",
        "\n",
        "        # Save per-seed metrics for this seed/K pair\n",
        "        seed_rows = []\n",
        "        for split_name, metrics in seed_results[K].items():\n",
        "            seed_rows.append({\n",
        "                'seed': seed,\n",
        "                'K': K,\n",
        "                'split': split_name,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1': metrics['f1'],\n",
        "                'auc': metrics['auc'],\n",
        "            })\n",
        "        seed_df = pd.DataFrame(seed_rows)\n",
        "        seed_metric_path = RESULTS_DIR / f'seed{seed}_k{K}_metrics.csv'\n",
        "        seed_df.to_csv(seed_metric_path, index=False)\n",
        "        print(f\"Saved metrics to {seed_metric_path}\")\n",
        "\n",
        "    all_seeds_results[seed] = seed_results\n",
        "    all_seeds_predictions[seed] = seed_predictions\n",
        "\n",
        "# Aggregate metrics across seeds\n",
        "all_rows = []\n",
        "for seed, seed_data in all_seeds_results.items():\n",
        "    for K, splits in seed_data.items():\n",
        "        for split_name, metrics in splits.items():\n",
        "            all_rows.append({\n",
        "                'seed': seed,\n",
        "                'K': K,\n",
        "                'split': split_name,\n",
        "                'accuracy': metrics['accuracy'],\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1': metrics['f1'],\n",
        "                'auc': metrics['auc'],\n",
        "            })\n",
        "\n",
        "if all_rows:\n",
        "    all_results_df = pd.DataFrame(all_rows)\n",
        "    all_results_path = RESULTS_DIR / 'all_seeds_all_metrics.csv'\n",
        "    all_results_df.to_csv(all_results_path, index=False)\n",
        "    print(f\"Saved aggregated metrics to {all_results_path}\")\n",
        "\n",
        "    summary_stats = []\n",
        "    for K in CONFIG['observation_windows']:\n",
        "        subset = all_results_df[(all_results_df['K'] == K) & (all_results_df['split'] == 'test')]\n",
        "        if subset.empty:\n",
        "            continue\n",
        "        summary_stats.append({\n",
        "            'K': K,\n",
        "            'f1_mean': subset['f1'].mean(),\n",
        "            'f1_std': subset['f1'].std(ddof=0),\n",
        "            'auc_mean': subset['auc'].mean(),\n",
        "            'auc_std': subset['auc'].std(ddof=0),\n",
        "            'precision_mean': subset['precision'].mean(),\n",
        "            'precision_std': subset['precision'].std(ddof=0),\n",
        "            'recall_mean': subset['recall'].mean(),\n",
        "            'recall_std': subset['recall'].std(ddof=0),\n",
        "            'accuracy_mean': subset['accuracy'].mean(),\n",
        "            'accuracy_std': subset['accuracy'].std(ddof=0),\n",
        "        })\n",
        "    summary_df = pd.DataFrame(summary_stats)\n",
        "    summary_path = RESULTS_DIR / 'multi_seed_summary_statistics.csv'\n",
        "    summary_df.to_csv(summary_path, index=False)\n",
        "    print(f\"Saved summary statistics to {summary_path}\")\n",
        "else:\n",
        "    summary_df = pd.DataFrame()\n",
        "\n",
        "if history_records:\n",
        "    history_df = pd.DataFrame(history_records)\n",
        "    history_path = RESULTS_DIR / 'training_history.csv'\n",
        "    history_df.to_csv(history_path, index=False)\n",
        "    print(f\"Saved training history to {history_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary_df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.11.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
