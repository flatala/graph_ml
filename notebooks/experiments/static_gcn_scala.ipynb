{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Scalability Analysis: Static Graph Node Classification\n",
    "\n",
    "This notebook demonstrates **scalable Graph Neural Network training** for Bitcoin fraud detection using **optimized neighborhood sampling strategies**. \n",
    "\n",
    "### üî¨ **Bitcoin Network Analysis**\n",
    "Based on degree distribution where:\n",
    "- 89.47% of nodes have ‚â§ 10 neighbors\n",
    "- 95.29% of nodes have ‚â§ 25 neighbors\n",
    "- Median degree: 2, Mean degree: 7\n",
    "- Hub nodes: Few nodes with 30K+ neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from code_lib.temporal_node_classification_builder import (\n",
    "    TemporalNodeClassificationBuilder,\n",
    "    load_elliptic_data,\n",
    "    prepare_observation_window_graphs\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_config import EXPERIMENT_CONFIG\n",
    "\n",
    "CONFIG = EXPERIMENT_CONFIG.copy()\n",
    "\n",
    "CONFIG['dropout'] = 0.3\n",
    "CONFIG['learning_rate'] = 0.001\n",
    "CONFIG['weight_decay'] = 1e-5\n",
    "CONFIG['epochs'] = 400\n",
    "CONFIG['patience'] = 20\n",
    "CONFIG['observation_windows']: [3, 5, 7]\n",
    "\n",
    "CONFIG['enable_sampling'] = True           # Enable neighborhood sampling\n",
    "CONFIG['batch_size'] = 2048                # Mini-batch size for target nodes\n",
    "CONFIG['num_workers'] = 4                  # Parallel data loading\n",
    "CONFIG['normalize'] = True                 # L2 normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature correlation removal function defined!\n"
     ]
    }
   ],
   "source": [
    "def remove_correlated_features(nodes_df, threshold=0.95, verbose=True):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features from nodes DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        nodes_df: DataFrame with node features\n",
    "        threshold: Correlation threshold (default 0.95)\n",
    "        verbose: Print removed features\n",
    "    \n",
    "    Returns:\n",
    "        list of kept feature columns\n",
    "    \"\"\"\n",
    "    # Identify feature columns (exclude address, Time step, class)\n",
    "    exclude_cols = {'address', 'Time step', 'class'}\n",
    "    feature_cols = [col for col in nodes_df.columns \n",
    "                    if col not in exclude_cols and \n",
    "                    pd.api.types.is_numeric_dtype(nodes_df[col])]\n",
    "    \n",
    "    # Compute correlation matrix on a sample (for speed)\n",
    "    sample_size = min(10000, len(nodes_df))\n",
    "    sample_df = nodes_df[feature_cols].sample(n=sample_size, random_state=42)\n",
    "    corr_matrix = sample_df.corr().abs()\n",
    "    \n",
    "    # Find features to remove\n",
    "    to_remove = set()\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if corr_matrix.iloc[i, j] > threshold:\n",
    "                # Remove the second feature (arbitrary choice)\n",
    "                feature_to_remove = corr_matrix.columns[j]\n",
    "                to_remove.add(feature_to_remove)\n",
    "                if verbose:\n",
    "                    print(f\"Removing {feature_to_remove} (corr={corr_matrix.iloc[i, j]:.3f} with {corr_matrix.columns[i]})\")\n",
    "    \n",
    "    # Keep features\n",
    "    features_to_keep = [col for col in feature_cols if col not in to_remove]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFeature reduction summary:\")\n",
    "        print(f\"  Original features: {len(feature_cols)}\")\n",
    "        print(f\"  Removed features:  {len(to_remove)}\")\n",
    "        print(f\"  Kept features:     {len(features_to_keep)}\")\n",
    "        print(f\"  Reduction ratio:   {len(to_remove)/len(feature_cols)*100:.1f}%\")\n",
    "    \n",
    "    return features_to_keep\n",
    "\n",
    "print(\"‚úÖ Feature correlation removal function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading Elliptic Bitcoin dataset...\n",
      "Loading trmporal features...\n",
      "Loading node classes...\n",
      "Loading edges...\n",
      "üìä Dataset loaded:\n",
      "  Nodes: 920,691 rows √ó 119 columns\n",
      "  Edges: 2,868,964 rows √ó 187 columns\n",
      "\n",
      "üîß Removing highly correlated features (threshold=0.95)...\n",
      "Removing out_num (corr=0.979 with in_num)\n",
      "Removing in_fees_sum (corr=1.000 with in_total_fees)\n",
      "Removing in_median_fees (corr=0.999 with in_mean_fees)\n",
      "Removing in_fees_mean (corr=1.000 with in_mean_fees)\n",
      "Removing in_fees_median (corr=0.999 with in_mean_fees)\n",
      "Removing in_fees_mean (corr=0.999 with in_median_fees)\n",
      "Removing in_fees_median (corr=1.000 with in_median_fees)\n",
      "Removing in_total_BTC_sum (corr=1.000 with in_total_btc_in)\n",
      "Removing in_in_BTC_max_sum (corr=0.978 with in_total_btc_in)\n",
      "Removing in_in_BTC_total_sum (corr=1.000 with in_total_btc_in)\n",
      "Removing in_out_BTC_max_sum (corr=0.982 with in_total_btc_in)\n",
      "Removing in_out_BTC_total_sum (corr=1.000 with in_total_btc_in)\n",
      "Removing in_median_btc_in (corr=0.997 with in_mean_btc_in)\n",
      "Removing in_total_BTC_mean (corr=1.000 with in_mean_btc_in)\n",
      "Removing in_total_BTC_median (corr=0.997 with in_mean_btc_in)\n",
      "Removing in_in_BTC_min_mean (corr=0.982 with in_mean_btc_in)\n",
      "Removing in_in_BTC_min_median (corr=0.978 with in_mean_btc_in)\n",
      "Removing in_in_BTC_max_mean (corr=0.995 with in_mean_btc_in)\n",
      "Removing in_in_BTC_max_median (corr=0.992 with in_mean_btc_in)\n",
      "Removing in_in_BTC_mean_mean (corr=0.988 with in_mean_btc_in)\n",
      "Removing in_in_BTC_mean_median (corr=0.985 with in_mean_btc_in)\n",
      "Removing in_in_BTC_median_mean (corr=0.988 with in_mean_btc_in)\n",
      "Removing in_in_BTC_median_median (corr=0.985 with in_mean_btc_in)\n",
      "Removing in_in_BTC_total_mean (corr=1.000 with in_mean_btc_in)\n",
      "Removing in_in_BTC_total_median (corr=0.997 with in_mean_btc_in)\n",
      "Removing in_out_BTC_total_mean (corr=1.000 with in_mean_btc_in)\n",
      "Removing in_out_BTC_total_median (corr=0.997 with in_mean_btc_in)\n",
      "Removing in_total_BTC_mean (corr=0.997 with in_median_btc_in)\n",
      "Removing in_total_BTC_median (corr=1.000 with in_median_btc_in)\n",
      "Removing in_in_BTC_min_mean (corr=0.979 with in_median_btc_in)\n",
      "Removing in_in_BTC_min_median (corr=0.981 with in_median_btc_in)\n",
      "Removing in_in_BTC_max_mean (corr=0.992 with in_median_btc_in)\n",
      "Removing in_in_BTC_max_median (corr=0.995 with in_median_btc_in)\n",
      "Removing in_in_BTC_mean_mean (corr=0.985 with in_median_btc_in)\n",
      "Removing in_in_BTC_mean_median (corr=0.988 with in_median_btc_in)\n",
      "Removing in_in_BTC_median_mean (corr=0.985 with in_median_btc_in)\n",
      "Removing in_in_BTC_median_median (corr=0.988 with in_median_btc_in)\n",
      "Removing in_in_BTC_total_mean (corr=0.997 with in_median_btc_in)\n",
      "Removing in_in_BTC_total_median (corr=1.000 with in_median_btc_in)\n",
      "Removing in_out_BTC_total_mean (corr=0.997 with in_median_btc_in)\n",
      "Removing in_out_BTC_total_median (corr=1.000 with in_median_btc_in)\n",
      "Removing in_fees_median (corr=0.999 with in_fees_mean)\n",
      "Removing in_size_median (corr=0.996 with in_size_mean)\n",
      "Removing in_num_output_addresses_mean (corr=1.000 with in_size_mean)\n",
      "Removing in_num_output_addresses_median (corr=0.995 with in_size_mean)\n",
      "Removing in_num_output_addresses_mean (corr=0.995 with in_size_median)\n",
      "Removing in_num_output_addresses_median (corr=0.999 with in_size_median)\n",
      "Removing in_in_txs_degree_median (corr=0.992 with in_in_txs_degree_mean)\n",
      "Removing in_out_txs_degree_median (corr=0.988 with in_out_txs_degree_mean)\n",
      "Removing in_num_input_addresses_median (corr=0.999 with in_num_input_addresses_mean)\n",
      "Removing in_num_output_addresses_median (corr=0.996 with in_num_output_addresses_mean)\n",
      "Removing in_in_BTC_max_sum (corr=0.978 with in_total_BTC_sum)\n",
      "Removing in_in_BTC_total_sum (corr=1.000 with in_total_BTC_sum)\n",
      "Removing in_out_BTC_max_sum (corr=0.982 with in_total_BTC_sum)\n",
      "Removing in_out_BTC_total_sum (corr=1.000 with in_total_BTC_sum)\n",
      "Removing in_total_BTC_median (corr=0.997 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_min_mean (corr=0.982 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_min_median (corr=0.978 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_max_mean (corr=0.995 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_max_median (corr=0.992 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_mean_mean (corr=0.988 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_mean_median (corr=0.985 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_median_mean (corr=0.988 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_median_median (corr=0.985 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_total_mean (corr=1.000 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_total_median (corr=0.997 with in_total_BTC_mean)\n",
      "Removing in_out_BTC_total_mean (corr=1.000 with in_total_BTC_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.997 with in_total_BTC_mean)\n",
      "Removing in_in_BTC_min_mean (corr=0.979 with in_total_BTC_median)\n",
      "Removing in_in_BTC_min_median (corr=0.981 with in_total_BTC_median)\n",
      "Removing in_in_BTC_max_mean (corr=0.992 with in_total_BTC_median)\n",
      "Removing in_in_BTC_max_median (corr=0.995 with in_total_BTC_median)\n",
      "Removing in_in_BTC_mean_mean (corr=0.985 with in_total_BTC_median)\n",
      "Removing in_in_BTC_mean_median (corr=0.988 with in_total_BTC_median)\n",
      "Removing in_in_BTC_median_mean (corr=0.985 with in_total_BTC_median)\n",
      "Removing in_in_BTC_median_median (corr=0.988 with in_total_BTC_median)\n",
      "Removing in_in_BTC_total_mean (corr=0.997 with in_total_BTC_median)\n",
      "Removing in_in_BTC_total_median (corr=1.000 with in_total_BTC_median)\n",
      "Removing in_out_BTC_total_mean (corr=0.997 with in_total_BTC_median)\n",
      "Removing in_out_BTC_total_median (corr=1.000 with in_total_BTC_median)\n",
      "Removing in_in_BTC_mean_sum (corr=0.999 with in_in_BTC_min_sum)\n",
      "Removing in_in_BTC_median_sum (corr=0.999 with in_in_BTC_min_sum)\n",
      "Removing out_total_btc_out (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_total_BTC_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_in_BTC_min_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_in_BTC_max_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_in_BTC_mean_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_in_BTC_median_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_in_BTC_total_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_out_BTC_max_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.988 with in_in_BTC_min_sum)\n",
      "Removing out_out_BTC_total_sum (corr=0.982 with in_in_BTC_min_sum)\n",
      "Removing in_in_BTC_min_median (corr=0.997 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_max_mean (corr=0.991 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_max_median (corr=0.988 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_mean_mean (corr=0.998 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_mean_median (corr=0.995 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_median_mean (corr=0.998 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_median_median (corr=0.995 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_total_mean (corr=0.982 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_total_median (corr=0.979 with in_in_BTC_min_mean)\n",
      "Removing in_out_BTC_total_mean (corr=0.982 with in_in_BTC_min_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.979 with in_in_BTC_min_mean)\n",
      "Removing in_in_BTC_max_mean (corr=0.987 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_max_median (corr=0.991 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_mean_mean (corr=0.995 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_mean_median (corr=0.998 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_median_mean (corr=0.995 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_median_median (corr=0.998 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_total_mean (corr=0.978 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_total_median (corr=0.981 with in_in_BTC_min_median)\n",
      "Removing in_out_BTC_total_mean (corr=0.978 with in_in_BTC_min_median)\n",
      "Removing in_out_BTC_total_median (corr=0.981 with in_in_BTC_min_median)\n",
      "Removing in_in_BTC_total_sum (corr=0.978 with in_in_BTC_max_sum)\n",
      "Removing in_out_BTC_max_sum (corr=0.977 with in_in_BTC_max_sum)\n",
      "Removing in_out_BTC_total_sum (corr=0.978 with in_in_BTC_max_sum)\n",
      "Removing in_in_BTC_max_median (corr=0.997 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_mean_mean (corr=0.996 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_mean_median (corr=0.992 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_median_mean (corr=0.996 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_median_median (corr=0.992 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_total_mean (corr=0.995 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_total_median (corr=0.992 with in_in_BTC_max_mean)\n",
      "Removing in_out_BTC_total_mean (corr=0.995 with in_in_BTC_max_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.992 with in_in_BTC_max_mean)\n",
      "Removing in_in_BTC_mean_mean (corr=0.993 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_mean_median (corr=0.996 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_median_mean (corr=0.993 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_median_median (corr=0.995 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_total_mean (corr=0.992 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_total_median (corr=0.995 with in_in_BTC_max_median)\n",
      "Removing in_out_BTC_total_mean (corr=0.992 with in_in_BTC_max_median)\n",
      "Removing in_out_BTC_total_median (corr=0.995 with in_in_BTC_max_median)\n",
      "Removing in_in_BTC_median_sum (corr=1.000 with in_in_BTC_mean_sum)\n",
      "Removing out_total_btc_out (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_total_BTC_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_min_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_max_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_mean_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_median_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_total_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_max_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.990 with in_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_total_sum (corr=0.983 with in_in_BTC_mean_sum)\n",
      "Removing in_in_BTC_mean_median (corr=0.997 with in_in_BTC_mean_mean)\n",
      "Removing in_in_BTC_median_mean (corr=1.000 with in_in_BTC_mean_mean)\n",
      "Removing in_in_BTC_median_median (corr=0.997 with in_in_BTC_mean_mean)\n",
      "Removing in_in_BTC_total_mean (corr=0.988 with in_in_BTC_mean_mean)\n",
      "Removing in_in_BTC_total_median (corr=0.985 with in_in_BTC_mean_mean)\n",
      "Removing in_out_BTC_total_mean (corr=0.988 with in_in_BTC_mean_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.985 with in_in_BTC_mean_mean)\n",
      "Removing in_in_BTC_median_mean (corr=0.997 with in_in_BTC_mean_median)\n",
      "Removing in_in_BTC_median_median (corr=1.000 with in_in_BTC_mean_median)\n",
      "Removing in_in_BTC_total_mean (corr=0.985 with in_in_BTC_mean_median)\n",
      "Removing in_in_BTC_total_median (corr=0.988 with in_in_BTC_mean_median)\n",
      "Removing in_out_BTC_total_mean (corr=0.985 with in_in_BTC_mean_median)\n",
      "Removing in_out_BTC_total_median (corr=0.988 with in_in_BTC_mean_median)\n",
      "Removing out_total_btc_out (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_total_BTC_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_in_BTC_min_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_in_BTC_max_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_in_BTC_mean_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_in_BTC_median_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_in_BTC_total_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing out_out_BTC_max_sum (corr=0.984 with in_in_BTC_median_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.990 with in_in_BTC_median_sum)\n",
      "Removing out_out_BTC_total_sum (corr=0.983 with in_in_BTC_median_sum)\n",
      "Removing in_in_BTC_median_median (corr=0.997 with in_in_BTC_median_mean)\n",
      "Removing in_in_BTC_total_mean (corr=0.988 with in_in_BTC_median_mean)\n",
      "Removing in_in_BTC_total_median (corr=0.985 with in_in_BTC_median_mean)\n",
      "Removing in_out_BTC_total_mean (corr=0.988 with in_in_BTC_median_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.985 with in_in_BTC_median_mean)\n",
      "Removing in_in_BTC_total_mean (corr=0.985 with in_in_BTC_median_median)\n",
      "Removing in_in_BTC_total_median (corr=0.988 with in_in_BTC_median_median)\n",
      "Removing in_out_BTC_total_mean (corr=0.985 with in_in_BTC_median_median)\n",
      "Removing in_out_BTC_total_median (corr=0.988 with in_in_BTC_median_median)\n",
      "Removing in_out_BTC_max_sum (corr=0.982 with in_in_BTC_total_sum)\n",
      "Removing in_out_BTC_total_sum (corr=1.000 with in_in_BTC_total_sum)\n",
      "Removing in_in_BTC_total_median (corr=0.997 with in_in_BTC_total_mean)\n",
      "Removing in_out_BTC_total_mean (corr=1.000 with in_in_BTC_total_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.997 with in_in_BTC_total_mean)\n",
      "Removing in_out_BTC_total_mean (corr=0.997 with in_in_BTC_total_median)\n",
      "Removing in_out_BTC_total_median (corr=1.000 with in_in_BTC_total_median)\n",
      "Removing in_out_BTC_min_mean (corr=0.980 with in_out_BTC_min_sum)\n",
      "Removing in_out_BTC_min_median (corr=0.980 with in_out_BTC_min_sum)\n",
      "Removing in_out_BTC_mean_sum (corr=0.968 with in_out_BTC_min_sum)\n",
      "Removing in_out_BTC_median_sum (corr=0.978 with in_out_BTC_min_sum)\n",
      "Removing in_out_BTC_min_median (corr=1.000 with in_out_BTC_min_mean)\n",
      "Removing in_out_BTC_median_sum (corr=0.955 with in_out_BTC_min_mean)\n",
      "Removing in_out_BTC_median_sum (corr=0.955 with in_out_BTC_min_median)\n",
      "Removing in_out_BTC_total_sum (corr=0.982 with in_out_BTC_max_sum)\n",
      "Removing in_out_BTC_max_median (corr=0.999 with in_out_BTC_max_mean)\n",
      "Removing in_out_BTC_median_sum (corr=0.991 with in_out_BTC_mean_sum)\n",
      "Removing in_out_BTC_mean_median (corr=1.000 with in_out_BTC_mean_mean)\n",
      "Removing in_out_BTC_median_mean (corr=0.983 with in_out_BTC_mean_mean)\n",
      "Removing in_out_BTC_median_median (corr=0.983 with in_out_BTC_mean_mean)\n",
      "Removing in_out_BTC_median_mean (corr=0.983 with in_out_BTC_mean_median)\n",
      "Removing in_out_BTC_median_median (corr=0.983 with in_out_BTC_mean_median)\n",
      "Removing in_out_BTC_median_median (corr=1.000 with in_out_BTC_median_mean)\n",
      "Removing in_out_BTC_total_median (corr=0.997 with in_out_BTC_total_mean)\n",
      "Removing out_fees_sum (corr=1.000 with out_total_fees)\n",
      "Removing out_median_fees (corr=1.000 with out_mean_fees)\n",
      "Removing out_fees_mean (corr=1.000 with out_mean_fees)\n",
      "Removing out_fees_median (corr=1.000 with out_mean_fees)\n",
      "Removing out_fees_mean (corr=1.000 with out_median_fees)\n",
      "Removing out_fees_median (corr=1.000 with out_median_fees)\n",
      "Removing out_total_BTC_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_in_BTC_min_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_in_BTC_max_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_in_BTC_mean_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_in_BTC_median_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_total_btc_out)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_total_btc_out)\n",
      "Removing out_median_btc_out (corr=0.984 with out_mean_btc_out)\n",
      "Removing out_total_BTC_mean (corr=1.000 with out_mean_btc_out)\n",
      "Removing out_total_BTC_median (corr=0.984 with out_mean_btc_out)\n",
      "Removing out_in_BTC_total_mean (corr=1.000 with out_mean_btc_out)\n",
      "Removing out_in_BTC_total_median (corr=0.984 with out_mean_btc_out)\n",
      "Removing out_out_BTC_max_mean (corr=0.989 with out_mean_btc_out)\n",
      "Removing out_out_BTC_max_median (corr=0.973 with out_mean_btc_out)\n",
      "Removing out_out_BTC_total_mean (corr=1.000 with out_mean_btc_out)\n",
      "Removing out_out_BTC_total_median (corr=0.984 with out_mean_btc_out)\n",
      "Removing out_total_BTC_mean (corr=0.984 with out_median_btc_out)\n",
      "Removing out_total_BTC_median (corr=1.000 with out_median_btc_out)\n",
      "Removing out_in_BTC_total_mean (corr=0.984 with out_median_btc_out)\n",
      "Removing out_in_BTC_total_median (corr=1.000 with out_median_btc_out)\n",
      "Removing out_out_BTC_max_mean (corr=0.973 with out_median_btc_out)\n",
      "Removing out_out_BTC_max_median (corr=0.989 with out_median_btc_out)\n",
      "Removing out_out_BTC_total_mean (corr=0.984 with out_median_btc_out)\n",
      "Removing out_out_BTC_total_median (corr=1.000 with out_median_btc_out)\n",
      "Removing out_fees_median (corr=1.000 with out_fees_mean)\n",
      "Removing out_size_median (corr=0.999 with out_size_mean)\n",
      "Removing out_num_input_addresses_mean (corr=0.988 with out_size_mean)\n",
      "Removing out_num_input_addresses_median (corr=0.986 with out_size_mean)\n",
      "Removing out_num_input_addresses_mean (corr=0.987 with out_size_median)\n",
      "Removing out_num_input_addresses_median (corr=0.988 with out_size_median)\n",
      "Removing out_in_txs_degree_median (corr=0.999 with out_in_txs_degree_mean)\n",
      "Removing out_out_txs_degree_median (corr=0.999 with out_out_txs_degree_mean)\n",
      "Removing out_num_input_addresses_median (corr=0.999 with out_num_input_addresses_mean)\n",
      "Removing out_num_output_addresses_median (corr=0.998 with out_num_output_addresses_mean)\n",
      "Removing out_in_BTC_min_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_in_BTC_max_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_in_BTC_mean_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_in_BTC_median_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_total_BTC_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_total_BTC_sum)\n",
      "Removing out_total_BTC_median (corr=0.984 with out_total_BTC_mean)\n",
      "Removing out_in_BTC_total_mean (corr=1.000 with out_total_BTC_mean)\n",
      "Removing out_in_BTC_total_median (corr=0.984 with out_total_BTC_mean)\n",
      "Removing out_out_BTC_max_mean (corr=0.989 with out_total_BTC_mean)\n",
      "Removing out_out_BTC_max_median (corr=0.973 with out_total_BTC_mean)\n",
      "Removing out_out_BTC_total_mean (corr=1.000 with out_total_BTC_mean)\n",
      "Removing out_out_BTC_total_median (corr=0.984 with out_total_BTC_mean)\n",
      "Removing out_in_BTC_total_mean (corr=0.984 with out_total_BTC_median)\n",
      "Removing out_in_BTC_total_median (corr=1.000 with out_total_BTC_median)\n",
      "Removing out_out_BTC_max_mean (corr=0.973 with out_total_BTC_median)\n",
      "Removing out_out_BTC_max_median (corr=0.989 with out_total_BTC_median)\n",
      "Removing out_out_BTC_total_mean (corr=0.984 with out_total_BTC_median)\n",
      "Removing out_out_BTC_total_median (corr=1.000 with out_total_BTC_median)\n",
      "Removing out_in_BTC_max_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_in_BTC_mean_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_in_BTC_median_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_in_BTC_min_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_in_BTC_min_sum)\n",
      "Removing out_in_BTC_min_median (corr=0.998 with out_in_BTC_min_mean)\n",
      "Removing out_in_BTC_mean_mean (corr=0.983 with out_in_BTC_min_mean)\n",
      "Removing out_in_BTC_mean_median (corr=0.981 with out_in_BTC_min_mean)\n",
      "Removing out_in_BTC_median_mean (corr=0.999 with out_in_BTC_min_mean)\n",
      "Removing out_in_BTC_median_median (corr=0.998 with out_in_BTC_min_mean)\n",
      "Removing out_in_BTC_mean_mean (corr=0.980 with out_in_BTC_min_median)\n",
      "Removing out_in_BTC_mean_median (corr=0.982 with out_in_BTC_min_median)\n",
      "Removing out_in_BTC_median_mean (corr=0.997 with out_in_BTC_min_median)\n",
      "Removing out_in_BTC_median_median (corr=0.999 with out_in_BTC_min_median)\n",
      "Removing out_in_BTC_mean_sum (corr=1.000 with out_in_BTC_max_sum)\n",
      "Removing out_in_BTC_median_sum (corr=1.000 with out_in_BTC_max_sum)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_in_BTC_max_sum)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_in_BTC_max_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_in_BTC_max_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_in_BTC_max_sum)\n",
      "Removing out_in_BTC_max_median (corr=0.981 with out_in_BTC_max_mean)\n",
      "Removing out_in_BTC_median_sum (corr=1.000 with out_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_in_BTC_mean_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_in_BTC_mean_sum)\n",
      "Removing out_in_BTC_mean_median (corr=0.998 with out_in_BTC_mean_mean)\n",
      "Removing out_in_BTC_median_mean (corr=0.984 with out_in_BTC_mean_mean)\n",
      "Removing out_in_BTC_median_median (corr=0.982 with out_in_BTC_mean_mean)\n",
      "Removing out_in_BTC_median_mean (corr=0.982 with out_in_BTC_mean_median)\n",
      "Removing out_in_BTC_median_median (corr=0.983 with out_in_BTC_mean_median)\n",
      "Removing out_in_BTC_total_sum (corr=1.000 with out_in_BTC_median_sum)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_in_BTC_median_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_in_BTC_median_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_in_BTC_median_sum)\n",
      "Removing out_in_BTC_median_median (corr=0.998 with out_in_BTC_median_mean)\n",
      "Removing out_out_BTC_max_sum (corr=1.000 with out_in_BTC_total_sum)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_in_BTC_total_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_in_BTC_total_sum)\n",
      "Removing out_in_BTC_total_median (corr=0.984 with out_in_BTC_total_mean)\n",
      "Removing out_out_BTC_max_mean (corr=0.989 with out_in_BTC_total_mean)\n",
      "Removing out_out_BTC_max_median (corr=0.973 with out_in_BTC_total_mean)\n",
      "Removing out_out_BTC_total_mean (corr=1.000 with out_in_BTC_total_mean)\n",
      "Removing out_out_BTC_total_median (corr=0.984 with out_in_BTC_total_mean)\n",
      "Removing out_out_BTC_max_mean (corr=0.973 with out_in_BTC_total_median)\n",
      "Removing out_out_BTC_max_median (corr=0.989 with out_in_BTC_total_median)\n",
      "Removing out_out_BTC_total_mean (corr=0.984 with out_in_BTC_total_median)\n",
      "Removing out_out_BTC_total_median (corr=1.000 with out_in_BTC_total_median)\n",
      "Removing out_out_BTC_min_median (corr=0.998 with out_out_BTC_min_mean)\n",
      "Removing out_out_BTC_mean_sum (corr=0.992 with out_out_BTC_max_sum)\n",
      "Removing out_out_BTC_total_sum (corr=1.000 with out_out_BTC_max_sum)\n",
      "Removing out_out_BTC_max_median (corr=0.984 with out_out_BTC_max_mean)\n",
      "Removing out_out_BTC_total_mean (corr=0.989 with out_out_BTC_max_mean)\n",
      "Removing out_out_BTC_total_median (corr=0.973 with out_out_BTC_max_mean)\n",
      "Removing out_out_BTC_total_mean (corr=0.973 with out_out_BTC_max_median)\n",
      "Removing out_out_BTC_total_median (corr=0.989 with out_out_BTC_max_median)\n",
      "Removing out_out_BTC_total_sum (corr=0.992 with out_out_BTC_mean_sum)\n",
      "Removing out_out_BTC_mean_median (corr=0.997 with out_out_BTC_mean_mean)\n",
      "Removing out_out_BTC_median_mean (corr=0.971 with out_out_BTC_mean_mean)\n",
      "Removing out_out_BTC_median_median (corr=0.970 with out_out_BTC_mean_mean)\n",
      "Removing out_out_BTC_median_mean (corr=0.971 with out_out_BTC_mean_median)\n",
      "Removing out_out_BTC_median_median (corr=0.972 with out_out_BTC_mean_median)\n",
      "Removing out_out_BTC_median_median (corr=0.999 with out_out_BTC_median_mean)\n",
      "Removing out_out_BTC_total_median (corr=0.984 with out_out_BTC_total_mean)\n",
      "\n",
      "Feature reduction summary:\n",
      "  Original features: 116\n",
      "  Removed features:  80\n",
      "  Kept features:     36\n",
      "  Reduction ratio:   69.0%\n",
      "\n",
      "üèóÔ∏è  Creating temporal graph builder with 36 features...\n",
      "  Pre-processing node features by (address, timestep)...\n",
      "  Pre-processing edges by timestep...\n",
      "  Average new nodes per timestep: 16794.7\n",
      "Initialized TemporalNodeClassificationBuilder\n",
      "  Total nodes: 822942\n",
      "  Total edges: 2868964\n",
      "  Time steps: 1 to 49\n",
      "  Feature columns (36): ['in_num', 'in_total_fees', 'in_mean_fees', 'in_total_btc_in', 'in_mean_btc_in']...\n",
      "  Include class as feature: False\n",
      "  Add temporal features: True\n",
      "  Add edge weights: False\n",
      "\n",
      "üìä Creating temporal train/val/test split...\n",
      "\n",
      "Temporal Split Summary:\n",
      "  Train: timesteps 5-26, 104704 nodes\n",
      "    Illicit: 6698, Licit: 98006\n",
      "Training illicit ratio: 0.06397081295843521\n",
      "  Val:   timesteps 27-31, 11230 nodes\n",
      "    Illicit: 809, Licit: 10421\n",
      "Validation illicit ratio: 0.07203918076580587\n",
      "  Test:  timesteps 32-40, 45963 nodes\n",
      "    Illicit: 3682, Licit: 42281\n",
      "Test illicit ratio: 0.08010791288645215\n",
      "\n",
      "‚úÖ Data preparation complete:\n",
      "  Train: 104704 nodes\n",
      "  Val:   11230 nodes\n",
      "  Test:  45963 nodes\n",
      "  Features used: 36 (after correlation removal)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"üìÅ Loading Elliptic Bitcoin dataset...\")\n",
    "nodes_df, edges_df = load_elliptic_data(CONFIG['data_dir'], use_temporal_features=True)\n",
    "\n",
    "print(f\"üìä Dataset loaded:\")\n",
    "print(f\"  Nodes: {nodes_df.shape[0]:,} rows √ó {nodes_df.shape[1]} columns\")\n",
    "print(f\"  Edges: {edges_df.shape[0]:,} rows √ó {edges_df.shape[1]} columns\")\n",
    "\n",
    "# Remove highly correlated features to reduce dimensionality and improve performance\n",
    "print(f\"\\nüîß Removing highly correlated features (threshold=0.95)...\")\n",
    "kept_features = remove_correlated_features(nodes_df, threshold=0.95, verbose=True)\n",
    "\n",
    "# Create temporal graph builder with reduced feature set\n",
    "print(f\"\\nüèóÔ∏è  Creating temporal graph builder with {len(kept_features)} features...\")\n",
    "builder = TemporalNodeClassificationBuilder(\n",
    "    nodes_df=nodes_df,\n",
    "    edges_df=edges_df,\n",
    "    feature_cols=kept_features,  # Use only non-correlated features\n",
    "    include_class_as_feature=False,\n",
    "    add_temporal_features=True,\n",
    "    use_temporal_edge_decay=False,\n",
    "    cache_dir='../../graph_cache_reduced_features_fixed',  # New cache dir for reduced features\n",
    "    use_cache=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create temporal split\n",
    "print(f\"\\nüìä Creating temporal train/val/test split...\")\n",
    "split = builder.get_train_val_test_split(\n",
    "    train_timesteps=CONFIG['train_timesteps'],\n",
    "    val_timesteps=CONFIG['val_timesteps'],\n",
    "    test_timesteps=CONFIG['test_timesteps'],\n",
    "    filter_unknown=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete:\")\n",
    "print(f\"  Train: {len(split['train'])} nodes\")\n",
    "print(f\"  Val:   {len(split['val'])} nodes\")\n",
    "print(f\"  Test:  {len(split['test'])} nodes\")\n",
    "print(f\"  Features used: {len(kept_features)} (after correlation removal)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Per-Node Graphs\n",
    "\n",
    "Each node evaluated at t_first(v) + K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPARING OBSERVATION WINDOW GRAPHS (PER-NODE EVALUATION)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "K = 1 (Each node evaluated at t_first + 1)\n",
      "======================================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Nodes to evaluate: 104,704\n",
      "  Evaluation times: t=6 to t=27\n",
      "  Unique graphs needed: 22\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t6_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t7_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t8_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t9_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t10_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t11_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t12_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t13_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t14_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t15_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t16_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t17_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t18_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t19_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t20_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t21_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t22_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t23_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t24_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t25_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t26_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t27_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 104,704\n",
      "\n",
      "VAL split:\n",
      "  Nodes to evaluate: 11,230\n",
      "  Evaluation times: t=28 to t=32\n",
      "  Unique graphs needed: 5\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t28_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t29_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t30_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t31_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t32_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 11,230\n",
      "\n",
      "TEST split:\n",
      "  Nodes to evaluate: 45,963\n",
      "  Evaluation times: t=33 to t=41\n",
      "  Unique graphs needed: 9\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t33_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t34_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t35_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t36_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t37_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t38_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t39_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t40_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t41_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 45,963\n",
      "\n",
      "======================================================================\n",
      "K = 3 (Each node evaluated at t_first + 3)\n",
      "======================================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Nodes to evaluate: 104,704\n",
      "  Evaluation times: t=8 to t=29\n",
      "  Unique graphs needed: 22\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t8_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t9_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t10_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t11_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t12_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t13_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t14_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t15_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t16_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t17_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t18_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t19_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t20_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t21_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t22_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t23_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t24_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t25_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t26_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t27_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t28_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t29_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 104,704\n",
      "\n",
      "VAL split:\n",
      "  Nodes to evaluate: 11,230\n",
      "  Evaluation times: t=30 to t=34\n",
      "  Unique graphs needed: 5\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t30_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t31_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t32_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t33_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t34_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 11,230\n",
      "\n",
      "TEST split:\n",
      "  Nodes to evaluate: 45,963\n",
      "  Evaluation times: t=35 to t=43\n",
      "  Unique graphs needed: 9\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t35_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t36_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t37_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t38_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t39_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t40_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t41_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t42_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t43_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 45,963\n",
      "\n",
      "======================================================================\n",
      "K = 5 (Each node evaluated at t_first + 5)\n",
      "======================================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Nodes to evaluate: 104,704\n",
      "  Evaluation times: t=10 to t=31\n",
      "  Unique graphs needed: 22\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t10_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t11_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t12_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t13_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t14_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t15_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t16_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t17_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t18_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t19_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t20_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t21_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t22_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t23_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t24_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t25_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t26_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t27_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t28_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t29_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t30_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t31_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 104,704\n",
      "\n",
      "VAL split:\n",
      "  Nodes to evaluate: 11,230\n",
      "  Evaluation times: t=32 to t=36\n",
      "  Unique graphs needed: 5\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t32_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t33_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t34_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t35_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t36_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 11,230\n",
      "\n",
      "TEST split:\n",
      "  Nodes to evaluate: 45,963\n",
      "  Evaluation times: t=37 to t=45\n",
      "  Unique graphs needed: 9\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t37_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t38_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t39_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t40_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t41_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t42_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t43_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t44_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t45_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 45,963\n",
      "\n",
      "======================================================================\n",
      "K = 7 (Each node evaluated at t_first + 7)\n",
      "======================================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Nodes to evaluate: 104,704\n",
      "  Evaluation times: t=12 to t=33\n",
      "  Unique graphs needed: 22\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t12_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t13_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t14_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t15_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t16_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t17_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t18_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t19_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t20_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t21_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t22_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t23_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t24_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t25_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t26_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t27_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t28_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t29_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t30_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t31_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t32_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t33_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 104,704\n",
      "\n",
      "VAL split:\n",
      "  Nodes to evaluate: 11,230\n",
      "  Evaluation times: t=34 to t=38\n",
      "  Unique graphs needed: 5\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t34_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t35_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t36_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t37_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t38_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 11,230\n",
      "\n",
      "TEST split:\n",
      "  Nodes to evaluate: 45,963\n",
      "  Evaluation times: t=39 to t=47\n",
      "  Unique graphs needed: 9\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t39_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t40_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t41_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t42_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t43_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t44_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t45_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t46_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  ‚úÖ Loaded cached graph from ../../graph_cache_reduced_features_fixed/graph_t47_metaTrue_classFalse_tempTrue_weightsFalse.pt\n",
      "  Total eval nodes across all graphs: 45,963\n",
      "\n",
      "======================================================================\n",
      "‚úÖ PER-NODE OBSERVATION WINDOW GRAPHS PREPARED\n",
      "======================================================================\n",
      "\n",
      "Created graphs for 4 observation windows √ó 3 splits\n",
      "\n",
      "Usage (collect data from all graphs in split):\n",
      "  X_train = [g.x[g.eval_mask] for g in graphs[K]['train']['graphs'].values()]\n",
      "  X_train = torch.cat(X_train)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(CONFIG['device'])\n",
    "\n",
    "graphs = prepare_observation_window_graphs(\n",
    "    builder,\n",
    "    split['train'],\n",
    "    split['val'],\n",
    "    split['test'],\n",
    "    K_values=CONFIG['observation_windows'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All model classes defined!\n",
      "Available models: standard_gcn, sampled_gcn\n"
     ]
    }
   ],
   "source": [
    "class StandardGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard GCN without sampling - traditional full graph approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        print(f\"Standard GCN initialized (no sampling)\")\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SampledGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    GCN with neighborhood sampling for scalability.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        print(f\"Sampled GCN initialized (with neighborhood sampling)\")\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Standard forward for full graphs\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def forward_sampled(self, x, adjs):\n",
    "        \"\"\"Forward pass for sampled subgraphs from NeighborSampler.\"\"\"\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]\n",
    "            if i == 0:\n",
    "                x = self.conv1(x, edge_index)\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            else:\n",
    "                x = self.conv2(x, edge_index)\n",
    "            x = x[:size[1]]  # Keep only target nodes\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model factory function\n",
    "def create_model(model_type, num_features, hidden_dim, num_classes, \n",
    "                dropout=0.5, normalize=True):\n",
    "    \"\"\"Factory function to create different model types.\"\"\"\n",
    "    if model_type == \"standard_gcn\":\n",
    "        return StandardGCN(num_features, hidden_dim, num_classes, dropout)\n",
    "    elif model_type == \"sampled_gcn\":\n",
    "        return SampledGCN(num_features, hidden_dim, num_classes, dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "print(\"‚úÖ All model classes defined!\")\n",
    "print(\"Available models: standard_gcn, sampled_gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adaptive_sampling_strategy(graph):\n",
    "    \"\"\"\n",
    "    Calculate adaptive sampling strategy based on node degrees.\n",
    "    - Hubs (top 1%): 30 neighbors first layer, 15 neighbors second layer\n",
    "    - Medium (next 5%): 12 neighbors first layer, 6 neighbors second layer  \n",
    "    - Low (remaining 94%): 2 neighbors first layer, 1 neighbor second layer\n",
    "    \n",
    "    Args:\n",
    "        graph: PyTorch Geometric graph\n",
    "    \n",
    "    Returns:\n",
    "        Dict with sampling strategies for different node types\n",
    "    \"\"\"\n",
    "    from torch_geometric.utils import degree\n",
    "    \n",
    "    # Calculate node degrees\n",
    "    degrees = degree(graph.edge_index[0], graph.num_nodes)\n",
    "    \n",
    "    # Calculate thresholds - top 1% are hubs, next 5% are medium\n",
    "    hub_threshold = torch.quantile(degrees, 0.99).item()  # Top 1%\n",
    "    medium_threshold = torch.quantile(degrees, 0.95).item()  # Top 6% (1% hubs + 5% medium)\n",
    "    \n",
    "    # Create fixed sampling strategies\n",
    "    strategies = {\n",
    "        'low_degree': [2, 2],\n",
    "        'medium_degree': [8, 2],\n",
    "        'high_degree': [20, 5]\n",
    "    }\n",
    "    \n",
    "    # Count nodes in each category\n",
    "    high_degree_count = (degrees >= hub_threshold).sum().item()\n",
    "    medium_degree_count = ((degrees >= medium_threshold) & (degrees < hub_threshold)).sum().item()\n",
    "    low_degree_count = (degrees < medium_threshold).sum().item()\n",
    "    \n",
    "    analysis = {\n",
    "        'total_nodes': graph.num_nodes,\n",
    "        'hub_threshold': hub_threshold,\n",
    "        'medium_threshold': medium_threshold,\n",
    "        'max_degree': degrees.max().item(),\n",
    "        'min_degree': degrees.min().item(),\n",
    "        'low_degree_nodes': low_degree_count,\n",
    "        'medium_degree_nodes': medium_degree_count, \n",
    "        'high_degree_nodes': high_degree_count,\n",
    "        'strategies': strategies\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def create_hub_aware_samplers(graphs_dict, config, model_type):\n",
    "    \"\"\"\n",
    "    Create NeighborSamplers with hub-aware adaptive sampling.\n",
    "    Uses specific sampling strategies: Hubs (top 1%) get [30,15], Medium (next 5%) get [12,6], Low get [2,1].\n",
    "    \"\"\"\n",
    "    use_sampling = model_type in [\"sampled_gcn\"] and config['enable_sampling']\n",
    "    \n",
    "    if not use_sampling:\n",
    "        return {'graphs': graphs_dict, 'samplers': None, 'target_nodes': None, 'adaptive_info': None}\n",
    "    else:\n",
    "        samplers = {}\n",
    "        target_nodes_dict = {}\n",
    "        adaptive_analyses = {}\n",
    "                \n",
    "        for eval_t, graph in graphs_dict.items():\n",
    "            # Analyze graph and determine adaptive strategies\n",
    "            adaptive_analysis = calculate_adaptive_sampling_strategy(graph)\n",
    "            adaptive_analyses[eval_t] = adaptive_analysis\n",
    "            \n",
    "            # Use medium-degree strategy as the default sampler (balanced approach)\n",
    "            sampling_strategy = adaptive_analysis['strategies']['medium_degree']\n",
    "            \n",
    "            # Create target nodes (staying on CPU for NeighborSampler)\n",
    "            target_nodes = torch.where(graph.eval_mask)[0].cpu()\n",
    "            target_nodes_dict[eval_t] = target_nodes\n",
    "            \n",
    "            # Create sampler with adaptive strategy\n",
    "            from torch_geometric.loader import NeighborSampler\n",
    "            sampler = NeighborSampler(\n",
    "                graph.edge_index.cpu(),\n",
    "                sizes=sampling_strategy,  # Use adaptive sampling sizes\n",
    "                batch_size=config['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=config.get('num_workers', 4)\n",
    "            )\n",
    "            \n",
    "            samplers[eval_t] = sampler\n",
    "        \n",
    "        return {\n",
    "            'graphs': graphs_dict,\n",
    "            'samplers': samplers,\n",
    "            'target_nodes': target_nodes_dict,\n",
    "            'adaptive_info': adaptive_analyses\n",
    "        }\n",
    "\n",
    "\n",
    "def train_epoch_with_hub_aware_samplers(model, sampler_data, optimizer, criterion, config, model_type):\n",
    "    \"\"\"\n",
    "    Enhanced training function with hub-aware sampling insights.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0 \n",
    "    total_samples = 0\n",
    "    \n",
    "    total_sampling_time = 0\n",
    "    total_forward_backward_time = 0\n",
    "    \n",
    "    use_sampling = model_type in [\"sampled_gcn\"] and config['enable_sampling']\n",
    "    \n",
    "    if not use_sampling:\n",
    "        # Standard full graph training\n",
    "        for eval_t, graph in sampler_data['graphs'].items():\n",
    "            fb_start = time.time()\n",
    "            logits = model(graph.x, graph.edge_index)\n",
    "            loss = criterion(logits[graph.eval_mask], graph.y[graph.eval_mask])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_forward_backward_time += time.time() - fb_start\n",
    "            \n",
    "            pred = logits[graph.eval_mask].argmax(dim=1)\n",
    "            correct = (pred == graph.y[graph.eval_mask]).sum().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_samples += graph.eval_mask.sum().item()\n",
    "    else:\n",
    "        # Hub-aware sampled training using pre-built samplers\n",
    "        graphs = sampler_data['graphs']\n",
    "        samplers = sampler_data['samplers']\n",
    "        target_nodes_dict = sampler_data['target_nodes']\n",
    "        \n",
    "        for eval_t in graphs.keys():\n",
    "            graph = graphs[eval_t]\n",
    "            sampler = samplers[eval_t]\n",
    "            target_nodes = target_nodes_dict[eval_t]\n",
    "            \n",
    "            # Sample subgraphs (with hub-aware sampling sizes)\n",
    "            sampling_start = time.time()\n",
    "            for batch_size, n_id, adjs in [sampler.sample(target_nodes)]:\n",
    "                total_sampling_time += time.time() - sampling_start\n",
    "                \n",
    "                # Extract features for sampled nodes\n",
    "                x_batch = graph.x[n_id].to(graph.x.device)\n",
    "                y_batch = graph.y[target_nodes].to(graph.y.device)\n",
    "                \n",
    "                # Convert adjacency info for model\n",
    "                adjs = [(adj.edge_index.to(graph.x.device), adj.e_id, adj.size) for adj in adjs]\n",
    "                \n",
    "                # Forward and backward pass\n",
    "                fb_start = time.time()\n",
    "                if hasattr(model, 'forward_sampled'):\n",
    "                    logits = model.forward_sampled(x_batch, adjs)\n",
    "                else:\n",
    "                    # Use first adjacency for simple models\n",
    "                    edge_index = adjs[0][0] if adjs else torch.empty((2, 0), device=graph.x.device)\n",
    "                    logits = model(x_batch, edge_index)\n",
    "                \n",
    "                # Loss only on target nodes (first batch_size nodes)\n",
    "                target_logits = logits[:batch_size]\n",
    "                loss = criterion(target_logits, y_batch)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_forward_backward_time += time.time() - fb_start\n",
    "                \n",
    "                pred = target_logits.argmax(dim=1)\n",
    "                correct = (pred == y_batch).sum().item()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += correct\n",
    "                total_samples += batch_size\n",
    "    \n",
    "    # Store timing info\n",
    "    train_epoch_with_hub_aware_samplers.last_sampling_time = total_sampling_time\n",
    "    train_epoch_with_hub_aware_samplers.last_forward_backward_time = total_forward_backward_time\n",
    "    \n",
    "    if use_sampling:\n",
    "        avg_loss = total_loss / max(total_samples // config['batch_size'], 1) if total_samples > 0 else 0\n",
    "    else:\n",
    "        avg_loss = total_loss / len(sampler_data['graphs']) if len(sampler_data['graphs']) > 0 else 0\n",
    "        \n",
    "    avg_acc = total_correct / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate_with_hub_aware_samplers(model, sampler_data, config, model_type):\n",
    "    \"\"\"\n",
    "    Enhanced evaluation with hub-aware sampling insights.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    use_sampling = model_type in [\"sampled_gcn\"] and config['enable_sampling']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if not use_sampling:\n",
    "            # Standard evaluation\n",
    "            for eval_t, graph in sampler_data['graphs'].items():\n",
    "                logits = model(graph.x, graph.edge_index)\n",
    "                pred = logits[graph.eval_mask].argmax(dim=1).cpu().numpy()\n",
    "                true = graph.y[graph.eval_mask].cpu().numpy()\n",
    "                probs = F.softmax(logits[graph.eval_mask], dim=1)[:, 1].cpu().numpy()\n",
    "                \n",
    "                all_preds.append(pred)\n",
    "                all_labels.append(true)\n",
    "                all_probs.append(probs)\n",
    "        else:\n",
    "            # Hub-aware sampled evaluation\n",
    "            graphs = sampler_data['graphs']\n",
    "            samplers = sampler_data['samplers']\n",
    "            target_nodes_dict = sampler_data['target_nodes']\n",
    "            \n",
    "            for eval_t in graphs.keys():\n",
    "                graph = graphs[eval_t]\n",
    "                sampler = samplers[eval_t]\n",
    "                target_nodes = target_nodes_dict[eval_t]\n",
    "                \n",
    "                for batch_size, n_id, adjs in [sampler.sample(target_nodes)]:\n",
    "                    x_batch = graph.x[n_id].to(graph.x.device)\n",
    "                    adjs = [(adj.edge_index.to(graph.x.device), adj.e_id, adj.size) for adj in adjs]\n",
    "                    \n",
    "                    if hasattr(model, 'forward_sampled'):\n",
    "                        logits = model.forward_sampled(x_batch, adjs)\n",
    "                    else:\n",
    "                        edge_index = adjs[0][0] if adjs else torch.empty((2, 0), device=graph.x.device)\n",
    "                        logits = model(x_batch, edge_index)\n",
    "                    \n",
    "                    target_logits = logits[:batch_size]\n",
    "                    pred = target_logits.argmax(dim=1).cpu().numpy()\n",
    "                    probs = F.softmax(target_logits, dim=1)[:, 1].cpu().numpy()\n",
    "                    \n",
    "                    all_preds.append(pred)\n",
    "                    all_labels.append(graph.y[target_nodes].cpu().numpy())\n",
    "                    all_probs.append(probs)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.5\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SAMPLING MODEL: GCN + Hub-Aware Sampling\n",
      "================================================================================\n",
      "\n",
      "Model: GCN + Hub-Aware Sampling | K=1\n",
      "Sampled GCN initialized (with neighborhood sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa02e18e4d24494ad90d85faf5b3a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GCN + Hub-Aware Sampling K=1:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 61577.5348 | 0.0000   | 0.0000   | Train:0.18s\n",
      "   10    | 43856.1477 | 0.0839   | 0.0833   | Train:0.18s\n",
      "   15    | 41729.7219 | 0.1702   | 0.1622   | Train:0.18s\n",
      "   20    | 24138.0344 | 0.2513   | 0.2923   | Train:0.19s\n",
      "   25    | 27639.1446 | 0.3329   | 0.2799   | Train:0.18s\n",
      "   30    | 41396.1199 | 0.1896   | 0.1742   | Train:0.18s\n",
      "   35    | 20369.2356 | 0.2557   | 0.2616   | Train:0.19s\n",
      "   40    | 23104.1010 | 0.2626   | 0.2972   | Train:0.19s\n",
      "   45    | 11590.1577 | 0.2627   | 0.3028   | Train:0.19s\n",
      "   50    | 10906.6191 | 0.3009   | 0.4074   | Train:0.18s\n",
      "   55    | 11732.8206 | 0.3229   | 0.3800   | Train:0.19s\n",
      "   60    | 12084.5748 | 0.3120   | 0.4239   | Train:0.17s\n",
      "   65    | 9375.4122 | 0.3310   | 0.4309   | Train:0.17s\n",
      "   70    | 15595.0535 | 0.3939   | 0.3976   | Train:0.19s\n",
      "   75    | 5072.1798 | 0.3468   | 0.3717   | Train:0.17s\n",
      "   80    | 24628.1896 | 0.3861   | 0.3737   | Train:0.17s\n",
      "   85    | 14702.3040 | 0.3662   | 0.4222   | Train:0.18s\n",
      "   90    | 7536.5896 | 0.3179   | 0.4274   | Train:0.18s\n",
      "   95    | 8775.3480 | 0.3773   | 0.4257   | Train:0.19s\n",
      "   100   | 9802.8357 | 0.3766   | 0.3989   | Train:0.17s\n",
      "   105   | 12980.2960 | 0.3967   | 0.4045   | Train:0.19s\n",
      "   110   | 33266.7671 | 0.3361   | 0.3903   | Train:0.17s\n",
      "   115   | 13683.7126 | 0.2646   | 0.2957   | Train:0.20s\n",
      "   120   | 10374.1317 | 0.3393   | 0.4321   | Train:0.17s\n",
      "   125   | 9913.6585 | 0.3983   | 0.4092   | Train:0.17s\n",
      "   130   | 11799.6483 | 0.3387   | 0.3514   | Train:0.18s\n",
      "   135   | 6001.1849 | 0.3484   | 0.3633   | Train:0.17s\n",
      "   140   | 7693.1161 | 0.3267   | 0.3451   | Train:0.18s\n",
      "   145   | 6570.9671 | 0.3305   | 0.3033   | Train:0.19s\n",
      "   150   | 3920.3755 | 0.3620   | 0.3721   | Train:0.19s\n",
      "   155   | 4026.0511 | 0.3565   | 0.3679   | Train:0.19s\n",
      "   160   | 2822.2734 | 0.3596   | 0.3491   | Train:0.18s\n",
      "   165   | 2509.9300 | 0.3545   | 0.3623   | Train:0.18s\n",
      "   170   | 5447.2729 | 0.3717   | 0.3470   | Train:0.18s\n",
      "   175   | 1406.2499 | 0.3634   | 0.3542   | Train:0.17s\n",
      "   180   | 11274.4363 | 0.3816   | 0.3968   | Train:0.19s\n",
      "   185   | 4244.3858 | 0.3307   | 0.3168   | Train:0.18s\n",
      "   190   | 1550.7864 | 0.2976   | 0.2615   | Train:0.18s\n",
      "   195   | 365.7500 | 0.3351   | 0.2105   | Train:0.20s\n",
      "   200   | 305.8902 | 0.3292   | 0.1968   | Train:0.19s\n",
      "   205   | 227.9183 | 0.3443   | 0.2807   | Train:0.16s\n",
      "   210   | 1397.6968 | 0.3549   | 0.3549   | Train:0.18s\n",
      "   215   | 225.1192 | 0.2981   | 0.3158   | Train:0.18s\n",
      "   220   | 124.3927 | 0.3732   | 0.2650   | Train:0.19s\n",
      "\n",
      " Early stopping at epoch 220 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.3706, AUC=0.8670, Acc=0.8616, Loss=124.3927\n",
      "   Val:   F1=0.2636, AUC=0.6816, Acc=0.7587\n",
      "   Test:  F1=0.2752, AUC=0.7249, Acc=0.7400\n",
      "   üîß Hub-Aware Samplers: 1.46s (intelligent adaptive sampling!)\n",
      "      ‚Ä¢ Validation Phase: 8.6s (17.2% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.18s\n",
      "Model: GCN + Hub-Aware Sampling | K=3\n",
      "Sampled GCN initialized (with neighborhood sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d409f44dd778430ba673b24704543b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GCN + Hub-Aware Sampling K=3:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 37052.6434 | 0.1001   | 0.0975   | Train:0.17s\n",
      "   10    | 32279.8306 | 0.1434   | 0.1632   | Train:0.18s\n",
      "   15    | 33657.1293 | 0.2290   | 0.2559   | Train:0.17s\n",
      "   20    | 24339.2261 | 0.2485   | 0.2789   | Train:0.17s\n",
      "   25    | 15539.1328 | 0.2998   | 0.3705   | Train:0.19s\n",
      "   30    | 21674.3798 | 0.2494   | 0.2775   | Train:0.19s\n",
      "   35    | 16646.4706 | 0.2755   | 0.3348   | Train:0.17s\n",
      "   40    | 12861.1737 | 0.3032   | 0.3970   | Train:0.19s\n",
      "   45    | 23077.4868 | 0.3818   | 0.3913   | Train:0.19s\n",
      "   50    | 7889.7400 | 0.2868   | 0.3950   | Train:0.20s\n",
      "   55    | 24261.8546 | 0.2759   | 0.3789   | Train:0.19s\n",
      "   60    | 5489.8863 | 0.3074   | 0.3400   | Train:0.18s\n",
      "   65    | 5600.4217 | 0.3189   | 0.3855   | Train:0.19s\n",
      "   70    | 18561.9342 | 0.3686   | 0.4006   | Train:0.18s\n",
      "   75    | 4791.7257 | 0.3066   | 0.4167   | Train:0.17s\n",
      "   80    | 3098.3951 | 0.3315   | 0.3569   | Train:0.17s\n",
      "   85    | 7996.2305 | 0.3776   | 0.4222   | Train:0.18s\n",
      "   90    | 2627.0045 | 0.3768   | 0.4270   | Train:0.19s\n",
      "   95    | 1836.9497 | 0.4026   | 0.4007   | Train:0.17s\n",
      "   100   | 2403.6854 | 0.3952   | 0.4036   | Train:0.17s\n",
      "   105   | 754.1786 | 0.3566   | 0.3649   | Train:0.17s\n",
      "   110   | 3729.4563 | 0.3144   | 0.4056   | Train:0.17s\n",
      "   115   | 7011.2146 | 0.2192   | 0.3469   | Train:0.17s\n",
      "   120   | 5023.6122 | 0.2555   | 0.2853   | Train:0.17s\n",
      "   125   | 2108.5111 | 0.3498   | 0.4255   | Train:0.19s\n",
      "   130   | 6031.4162 | 0.3940   | 0.4204   | Train:0.18s\n",
      "   135   | 1077.4168 | 0.3727   | 0.2867   | Train:0.17s\n",
      "   140   | 547.1396 | 0.3742   | 0.3580   | Train:0.17s\n",
      "   145   | 342.3517 | 0.3763   | 0.3558   | Train:0.17s\n",
      "   150   | 1952.7869 | 0.3315   | 0.3132   | Train:0.17s\n",
      "   155   | 474.5586 | 0.3521   | 0.3195   | Train:0.17s\n",
      "   160   | 64.3253  | 0.3848   | 0.2816   | Train:0.18s\n",
      "   165   | 64.9938  | 0.4113   | 0.3372   | Train:0.19s\n",
      "   170   | 225.6601 | 0.3266   | 0.3865   | Train:0.17s\n",
      "   175   | 57.2334  | 0.3901   | 0.3170   | Train:0.17s\n",
      "   180   | 18.2256  | 0.4216   | 0.3357   | Train:0.17s\n",
      "   185   | 6.6394   | 0.3888   | 0.3131   | Train:0.17s\n",
      "   190   | 12.0784  | 0.4104   | 0.3080   | Train:0.17s\n",
      "\n",
      " Early stopping at epoch 190 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.4103, AUC=0.9067, Acc=0.8548, Loss=12.0784\n",
      "   Val:   F1=0.3106, AUC=0.6893, Acc=0.8012\n",
      "   Test:  F1=0.3567, AUC=0.8206, Acc=0.8014\n",
      "   üîß Hub-Aware Samplers: 1.49s (intelligent adaptive sampling!)\n",
      "      ‚Ä¢ Validation Phase: 7.3s (17.1% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.18s\n",
      "Model: GCN + Hub-Aware Sampling | K=5\n",
      "Sampled GCN initialized (with neighborhood sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a7e5a3f10e48cca462a909e12bff50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GCN + Hub-Aware Sampling K=5:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 44507.5624 | 0.0012   | 0.0072   | Train:0.17s\n",
      "   10    | 43664.4849 | 0.0009   | 0.0000   | Train:0.17s\n",
      "   15    | 27589.6859 | 0.2311   | 0.2550   | Train:0.17s\n",
      "   20    | 15681.5761 | 0.2890   | 0.3433   | Train:0.17s\n",
      "   25    | 9478.6184 | 0.2731   | 0.3325   | Train:0.17s\n",
      "   30    | 10400.2194 | 0.3034   | 0.3803   | Train:0.17s\n",
      "   35    | 18852.3216 | 0.2017   | 0.1910   | Train:0.18s\n",
      "   40    | 8758.2461 | 0.3138   | 0.3970   | Train:0.19s\n",
      "   45    | 31445.5554 | 0.2762   | 0.3649   | Train:0.18s\n",
      "   50    | 15762.4756 | 0.2827   | 0.3885   | Train:0.19s\n",
      "   55    | 13038.6755 | 0.3192   | 0.4198   | Train:0.17s\n",
      "   60    | 7528.5870 | 0.3312   | 0.3663   | Train:0.18s\n",
      "   65    | 9857.7002 | 0.3820   | 0.4213   | Train:0.17s\n",
      "   70    | 6103.3099 | 0.3515   | 0.4193   | Train:0.17s\n",
      "   75    | 2923.4236 | 0.3387   | 0.3556   | Train:0.18s\n",
      "   80    | 2994.5608 | 0.3323   | 0.3491   | Train:0.19s\n",
      "   85    | 3018.7126 | 0.3621   | 0.3733   | Train:0.18s\n",
      "   90    | 2059.9194 | 0.3608   | 0.3719   | Train:0.17s\n",
      "   95    | 3573.1609 | 0.3745   | 0.4208   | Train:0.17s\n",
      "   100   | 1898.7647 | 0.4015   | 0.4305   | Train:0.17s\n",
      "   105   | 1163.3007 | 0.3775   | 0.3552   | Train:0.17s\n",
      "   110   | 1263.5076 | 0.3540   | 0.4148   | Train:0.17s\n",
      "   115   | 504.3046 | 0.3759   | 0.3169   | Train:0.19s\n",
      "   120   | 5721.0279 | 0.2535   | 0.3540   | Train:0.17s\n",
      "   125   | 764.5827 | 0.3671   | 0.3711   | Train:0.18s\n",
      "   130   | 695.1809 | 0.3705   | 0.3731   | Train:0.17s\n",
      "   135   | 647.9630 | 0.3949   | 0.4004   | Train:0.18s\n",
      "   140   | 517.5944 | 0.4193   | 0.4419   | Train:0.20s\n",
      "   145   | 245.6843 | 0.3691   | 0.3700   | Train:0.19s\n",
      "   150   | 532.7849 | 0.3251   | 0.4232   | Train:0.19s\n",
      "   155   | 349.9749 | 0.3995   | 0.4453   | Train:0.17s\n",
      "   160   | 2177.9987 | 0.3888   | 0.4058   | Train:0.19s\n",
      "   165   | 596.7009 | 0.2828   | 0.3099   | Train:0.18s\n",
      "   170   | 175.9015 | 0.2932   | 0.3163   | Train:0.20s\n",
      "   175   | 79.4814  | 0.3008   | 0.3136   | Train:0.18s\n",
      "   180   | 265.9966 | 0.2912   | 0.3201   | Train:0.18s\n",
      "   185   | 41.2302  | 0.3097   | 0.3032   | Train:0.19s\n",
      "   190   | 29.7767  | 0.3452   | 0.3198   | Train:0.19s\n",
      "   195   | 51.8525  | 0.2907   | 0.3072   | Train:0.19s\n",
      "   200   | 12.3400  | 0.3682   | 0.3399   | Train:0.17s\n",
      "   205   | 13.4452  | 0.3275   | 0.3339   | Train:0.17s\n",
      "   210   | 10.8322  | 0.3722   | 0.3294   | Train:0.17s\n",
      "   215   | 17.1093  | 0.3779   | 0.3712   | Train:0.17s\n",
      "   220   | 17.4608  | 0.4068   | 0.4155   | Train:0.17s\n",
      "   225   | 16.3107  | 0.4322   | 0.4510   | Train:0.17s\n",
      "   230   | 70.3191  | 0.4673   | 0.3633   | Train:0.17s\n",
      "   235   | 1.5651   | 0.3717   | 0.3721   | Train:0.18s\n",
      "   240   | 0.5340   | 0.4040   | 0.5522   | Train:0.19s\n",
      "   245   | 0.6817   | 0.3567   | 0.3459   | Train:0.18s\n",
      "   250   | 0.3382   | 0.3175   | 0.5401   | Train:0.18s\n",
      "   255   | 0.3786   | 0.2344   | 0.3520   | Train:0.17s\n",
      "   260   | 0.4606   | 0.2133   | 0.3370   | Train:0.19s\n",
      "   265   | 0.3701   | 0.2028   | 0.3419   | Train:0.18s\n",
      "   270   | 0.6044   | 0.1782   | 0.3224   | Train:0.17s\n",
      "   275   | 0.6524   | 0.2249   | 0.3392   | Train:0.19s\n",
      "   280   | 0.2633   | 0.1696   | 0.3089   | Train:0.18s\n",
      "   285   | 0.2528   | 0.2993   | 0.3220   | Train:0.17s\n",
      "   290   | 0.2128   | 0.1628   | 0.3102   | Train:0.17s\n",
      "   295   | 0.2153   | 0.1951   | 0.3155   | Train:0.17s\n",
      "   300   | 0.1970   | 0.1688   | 0.3118   | Train:0.17s\n",
      "   305   | 0.1932   | 0.1819   | 0.3237   | Train:0.17s\n",
      "   310   | 0.2015   | 0.1647   | 0.3023   | Train:0.17s\n",
      "   315   | 0.2039   | 0.1784   | 0.3164   | Train:0.17s\n",
      "   320   | 0.1903   | 0.1965   | 0.3270   | Train:0.17s\n",
      "   325   | 0.2003   | 0.1780   | 0.3230   | Train:0.19s\n",
      "   330   | 0.1964   | 0.1758   | 0.3159   | Train:0.17s\n",
      "   335   | 0.2001   | 0.1890   | 0.3247   | Train:0.20s\n",
      "   340   | 0.1934   | 0.2003   | 0.3365   | Train:0.20s\n",
      "\n",
      " Early stopping at epoch 340 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.1975, AUC=0.8673, Acc=0.9396, Loss=0.1934\n",
      "   Val:   F1=0.3364, AUC=0.8615, Acc=0.9368\n",
      "   Test:  F1=0.1453, AUC=0.7942, Acc=0.9209\n",
      "   üîß Hub-Aware Samplers: 1.60s (intelligent adaptive sampling!)\n",
      "      ‚Ä¢ Validation Phase: 13.2s (17.3% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.18s\n",
      "Model: GCN + Hub-Aware Sampling | K=7\n",
      "Sampled GCN initialized (with neighborhood sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb50f854d6346d1b56a6d34d01e3705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GCN + Hub-Aware Sampling K=7:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 54090.3659 | 0.1562   | 0.1683   | Train:0.17s\n",
      "   10    | 42891.6339 | 0.1703   | 0.1742   | Train:0.19s\n",
      "   15    | 57490.4381 | 0.2800   | 0.3931   | Train:0.17s\n",
      "   20    | 31122.2235 | 0.2720   | 0.3715   | Train:0.17s\n",
      "   25    | 38974.1025 | 0.3090   | 0.4134   | Train:0.18s\n",
      "   30    | 26779.8333 | 0.3019   | 0.3881   | Train:0.18s\n",
      "   35    | 23483.4745 | 0.3020   | 0.3986   | Train:0.20s\n",
      "   40    | 27405.9635 | 0.3104   | 0.4024   | Train:0.20s\n",
      "   45    | 28447.3650 | 0.3407   | 0.5238   | Train:0.19s\n",
      "   50    | 21133.9089 | 0.3184   | 0.3983   | Train:0.20s\n",
      "   55    | 24846.2192 | 0.3194   | 0.4085   | Train:0.20s\n",
      "   60    | 14931.9690 | 0.3253   | 0.4820   | Train:0.17s\n",
      "   65    | 13922.7661 | 0.3075   | 0.4616   | Train:0.18s\n",
      "   70    | 16389.6867 | 0.3056   | 0.4272   | Train:0.17s\n",
      "   75    | 13146.2327 | 0.3261   | 0.4475   | Train:0.18s\n",
      "   80    | 11212.8791 | 0.3003   | 0.4774   | Train:0.20s\n",
      "   85    | 15844.6839 | 0.3398   | 0.5574   | Train:0.20s\n",
      "   90    | 14100.2594 | 0.3790   | 0.4615   | Train:0.19s\n",
      "   95    | 9860.7198 | 0.3622   | 0.4901   | Train:0.17s\n",
      "   100   | 6251.5859 | 0.3135   | 0.4250   | Train:0.20s\n",
      "   105   | 7865.2065 | 0.3403   | 0.4587   | Train:0.20s\n",
      "   110   | 37023.2581 | 0.3485   | 0.4130   | Train:0.20s\n",
      "   115   | 8889.7149 | 0.2082   | 0.2101   | Train:0.20s\n",
      "   120   | 7186.4092 | 0.3154   | 0.5013   | Train:0.20s\n",
      "   125   | 3740.1955 | 0.2940   | 0.3898   | Train:0.17s\n",
      "   130   | 4183.7420 | 0.3310   | 0.4286   | Train:0.17s\n",
      "   135   | 3092.9998 | 0.3048   | 0.3601   | Train:0.17s\n",
      "   140   | 3618.3630 | 0.3672   | 0.3817   | Train:0.19s\n",
      "   145   | 1680.7564 | 0.3100   | 0.3668   | Train:0.18s\n",
      "   150   | 1925.6279 | 0.3788   | 0.4348   | Train:0.18s\n",
      "   155   | 1228.3723 | 0.3966   | 0.5542   | Train:0.19s\n",
      "   160   | 1320.1504 | 0.3494   | 0.4314   | Train:0.20s\n",
      "   165   | 2576.4005 | 0.3739   | 0.3596   | Train:0.17s\n",
      "   170   | 1615.0932 | 0.2979   | 0.4164   | Train:0.20s\n",
      "   175   | 2759.0770 | 0.2062   | 0.2264   | Train:0.17s\n",
      "   180   | 1757.9116 | 0.3658   | 0.5105   | Train:0.18s\n",
      "   185   | 589.8809 | 0.3329   | 0.3381   | Train:0.18s\n",
      "\n",
      " Early stopping at epoch 185 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.3338, AUC=0.8410, Acc=0.8599, Loss=589.8809\n",
      "   Val:   F1=0.3435, AUC=0.7776, Acc=0.8250\n",
      "   Test:  F1=0.2848, AUC=0.7428, Acc=0.8258\n",
      "   üîß Hub-Aware Samplers: 1.76s (intelligent adaptive sampling!)\n",
      "      ‚Ä¢ Validation Phase: 7.3s (16.8% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.19s\n",
      "================================================================================\n",
      "TRAINING STANDARD MODEL: Standard GCN\n",
      "================================================================================\n",
      "\n",
      "Model: Standard GCN | K=1\n",
      "Standard GCN initialized (no sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f134c030bd3b49cf9b5e71ca15593967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standard GCN K=1:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 51402.8310 | 0.0000   | 0.0000   | Train:0.27s\n",
      "   10    | 58565.9487 | 0.0058   | 0.0049   | Train:0.27s\n",
      "   15    | 67317.5948 | 0.0525   | 0.0465   | Train:0.27s\n",
      "   20    | 70467.5591 | 0.1489   | 0.1445   | Train:0.27s\n",
      "   25    | 70717.7803 | 0.1833   | 0.1825   | Train:0.27s\n",
      "   30    | 53085.2605 | 0.2264   | 0.4212   | Train:0.27s\n",
      "   35    | 52860.2245 | 0.2771   | 0.3851   | Train:0.27s\n",
      "   40    | 62668.6749 | 0.2964   | 0.3716   | Train:0.27s\n",
      "   45    | 56769.5297 | 0.3063   | 0.5207   | Train:0.27s\n",
      "   50    | 42391.3046 | 0.3051   | 0.3993   | Train:0.27s\n",
      "   55    | 71897.6914 | 0.3464   | 0.4028   | Train:0.27s\n",
      "   60    | 42467.6530 | 0.3214   | 0.3683   | Train:0.27s\n",
      "   65    | 41360.0138 | 0.3257   | 0.4335   | Train:0.27s\n",
      "   70    | 39499.3436 | 0.3359   | 0.4554   | Train:0.27s\n",
      "   75    | 48306.3006 | 0.3376   | 0.5213   | Train:0.27s\n",
      "   80    | 53007.3708 | 0.3527   | 0.5121   | Train:0.27s\n",
      "   85    | 48928.5968 | 0.3576   | 0.4697   | Train:0.27s\n",
      "   90    | 81986.0768 | 0.3832   | 0.3689   | Train:0.27s\n",
      "   95    | 40777.9087 | 0.3386   | 0.3971   | Train:0.27s\n",
      "   100   | 50828.1437 | 0.3612   | 0.4891   | Train:0.27s\n",
      "   105   | 36589.1968 | 0.3689   | 0.4550   | Train:0.27s\n",
      "   110   | 31562.3219 | 0.3680   | 0.4657   | Train:0.27s\n",
      "   115   | 39543.4610 | 0.3717   | 0.4635   | Train:0.27s\n",
      "   120   | 36187.1151 | 0.3687   | 0.4593   | Train:0.27s\n",
      "   125   | 67257.6893 | 0.3622   | 0.4378   | Train:0.27s\n",
      "   130   | 31445.7444 | 0.3422   | 0.4138   | Train:0.27s\n",
      "   135   | 34988.4246 | 0.3697   | 0.4552   | Train:0.27s\n",
      "   140   | 25013.1602 | 0.3734   | 0.4609   | Train:0.27s\n",
      "   145   | 28303.0588 | 0.3925   | 0.3949   | Train:0.27s\n",
      "   150   | 31436.0623 | 0.3922   | 0.4883   | Train:0.27s\n",
      "   155   | 19263.5681 | 0.3518   | 0.4399   | Train:0.27s\n",
      "   160   | 23331.2409 | 0.3665   | 0.4388   | Train:0.27s\n",
      "   165   | 24575.5314 | 0.3597   | 0.4353   | Train:0.27s\n",
      "   170   | 17563.9366 | 0.3686   | 0.3534   | Train:0.27s\n",
      "   175   | 18824.5970 | 0.3780   | 0.3682   | Train:0.27s\n",
      "\n",
      " Early stopping at epoch 175 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.3780, AUC=0.7766, Acc=0.8855, Loss=18824.5970\n",
      "   Val:   F1=0.3682, AUC=0.7730, Acc=0.8512\n",
      "   Test:  F1=0.3289, AUC=0.6978, Acc=0.8535\n",
      "      ‚Ä¢ Validation Phase: 6.3s (11.9% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.27s\n",
      "Model: Standard GCN | K=3\n",
      "Standard GCN initialized (no sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770601bbaf24d3bae49efa4ea97f9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standard GCN K=3:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 107359.0556 | 0.0000   | 0.0000   | Train:0.29s\n",
      "   10    | 90126.0055 | 0.0000   | 0.0000   | Train:0.29s\n",
      "   15    | 64300.7272 | 0.0048   | 0.0147   | Train:0.29s\n",
      "   20    | 83410.5362 | 0.0985   | 0.1148   | Train:0.30s\n",
      "   25    | 100869.3650 | 0.1304   | 0.1504   | Train:0.29s\n",
      "   30    | 86438.0979 | 0.2104   | 0.3290   | Train:0.29s\n",
      "   35    | 77263.6370 | 0.2442   | 0.3323   | Train:0.29s\n",
      "   40    | 68483.2937 | 0.2816   | 0.4045   | Train:0.29s\n",
      "   45    | 59830.5387 | 0.2879   | 0.4070   | Train:0.29s\n",
      "   50    | 53735.9087 | 0.2960   | 0.4113   | Train:0.29s\n",
      "   55    | 93559.2822 | 0.2961   | 0.4127   | Train:0.29s\n",
      "   60    | 69203.8448 | 0.3052   | 0.3881   | Train:0.29s\n",
      "   65    | 55194.9578 | 0.3187   | 0.3966   | Train:0.29s\n",
      "   70    | 61528.4937 | 0.3189   | 0.3956   | Train:0.29s\n",
      "   75    | 73019.7436 | 0.3250   | 0.3982   | Train:0.29s\n",
      "   80    | 79089.5169 | 0.3576   | 0.4104   | Train:0.29s\n",
      "   85    | 63658.5590 | 0.3406   | 0.3800   | Train:0.29s\n",
      "   90    | 51994.7215 | 0.3438   | 0.3941   | Train:0.29s\n",
      "   95    | 58588.3429 | 0.3412   | 0.3772   | Train:0.29s\n",
      "   100   | 52280.2530 | 0.3575   | 0.3898   | Train:0.29s\n",
      "   105   | 61875.8645 | 0.3585   | 0.3752   | Train:0.29s\n",
      "   110   | 68496.1737 | 0.3699   | 0.3711   | Train:0.29s\n",
      "   115   | 56583.7969 | 0.3724   | 0.3689   | Train:0.29s\n",
      "   120   | 52289.3285 | 0.3756   | 0.3667   | Train:0.29s\n",
      "   125   | 57815.6716 | 0.3736   | 0.3587   | Train:0.29s\n",
      "   130   | 51061.7235 | 0.3734   | 0.3949   | Train:0.29s\n",
      "   135   | 44743.1887 | 0.3655   | 0.3442   | Train:0.29s\n",
      "   140   | 49761.0324 | 0.3628   | 0.3371   | Train:0.29s\n",
      "   145   | 46280.9954 | 0.3625   | 0.3560   | Train:0.30s\n",
      "   150   | 54239.9828 | 0.3615   | 0.3386   | Train:0.29s\n",
      "   155   | 37005.7762 | 0.3605   | 0.3406   | Train:0.29s\n",
      "\n",
      " Early stopping at epoch 155 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.3605, AUC=0.7276, Acc=0.8806, Loss=37005.7762\n",
      "   Val:   F1=0.3406, AUC=0.7262, Acc=0.8438\n",
      "   Test:  F1=0.3116, AUC=0.6607, Acc=0.8496\n",
      "      ‚Ä¢ Validation Phase: 5.9s (11.5% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.29s\n",
      "Model: Standard GCN | K=5\n",
      "Standard GCN initialized (no sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206fbf9458ca4d0c9654852153f90f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standard GCN K=5:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 120380.4353 | 0.0000   | 0.0000   | Train:0.31s\n",
      "   10    | 59624.2837 | 0.1750   | 0.1711   | Train:0.31s\n",
      "   15    | 122598.3028 | 0.0000   | 0.0000   | Train:0.31s\n",
      "   20    | 87312.5943 | 0.3220   | 0.4050   | Train:0.31s\n",
      "   25    | 68092.9057 | 0.2909   | 0.3936   | Train:0.32s\n",
      "   30    | 46955.7226 | 0.3049   | 0.3596   | Train:0.31s\n",
      "   35    | 92402.2523 | 0.2655   | 0.3774   | Train:0.31s\n",
      "   40    | 60115.4649 | 0.3058   | 0.3322   | Train:0.31s\n",
      "   45    | 53277.6104 | 0.3177   | 0.3914   | Train:0.31s\n",
      "   50    | 104368.8704 | 0.2534   | 0.3752   | Train:0.31s\n",
      "   55    | 37040.7036 | 0.3010   | 0.3319   | Train:0.31s\n",
      "   60    | 37291.1923 | 0.3163   | 0.3499   | Train:0.31s\n",
      "   65    | 36446.9092 | 0.3294   | 0.3507   | Train:0.31s\n",
      "   70    | 36654.9819 | 0.3138   | 0.3407   | Train:0.31s\n",
      "   75    | 36334.7922 | 0.3474   | 0.3496   | Train:0.31s\n",
      "   80    | 33544.5985 | 0.3414   | 0.3636   | Train:0.31s\n",
      "   85    | 35670.1501 | 0.3580   | 0.3437   | Train:0.31s\n",
      "   90    | 29552.2027 | 0.3555   | 0.3579   | Train:0.31s\n",
      "   95    | 26623.6418 | 0.3274   | 0.3228   | Train:0.31s\n",
      "   100   | 45772.9628 | 0.3472   | 0.3232   | Train:0.31s\n",
      "   105   | 25052.2522 | 0.3491   | 0.3601   | Train:0.31s\n",
      "   110   | 15653.3313 | 0.3459   | 0.3705   | Train:0.31s\n",
      "   115   | 17196.0184 | 0.3739   | 0.3615   | Train:0.31s\n",
      "   120   | 15896.8227 | 0.3522   | 0.3534   | Train:0.31s\n",
      "\n",
      " Early stopping at epoch 120 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.3522, AUC=0.7434, Acc=0.8776, Loss=15896.8227\n",
      "   Val:   F1=0.3534, AUC=0.7605, Acc=0.8378\n",
      "   Test:  F1=0.3130, AUC=0.6897, Acc=0.8458\n",
      "      ‚Ä¢ Validation Phase: 4.9s (11.5% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.31s\n",
      "Model: Standard GCN | K=7\n",
      "Standard GCN initialized (no sampling)\n",
      "   üîß Creating hub-aware adaptive samplers once for entire training...\n",
      "   Epoch | Loss     | Train F1 | Val F1   | Epoch Time | Details\n",
      "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7f5a934fa4440ca19a9f9d04a88259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Standard GCN K=7:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5     | 53329.2773 | 0.0988   | 0.1080   | Train:0.33s\n",
      "   10    | 41126.8267 | 0.2401   | 0.2651   | Train:0.34s\n",
      "   15    | 41062.6192 | 0.2483   | 0.2775   | Train:0.33s\n",
      "   20    | 32053.5088 | 0.3033   | 0.3839   | Train:0.33s\n",
      "   25    | 25801.3603 | 0.3058   | 0.4095   | Train:0.33s\n",
      "   30    | 16990.5534 | 0.3242   | 0.3780   | Train:0.33s\n",
      "   35    | 13718.9844 | 0.3038   | 0.4153   | Train:0.33s\n",
      "   40    | 31969.3183 | 0.1395   | 0.1432   | Train:0.33s\n",
      "   45    | 12028.8028 | 0.3223   | 0.5819   | Train:0.33s\n",
      "   50    | 5553.9334 | 0.3176   | 0.5128   | Train:0.34s\n",
      "   55    | 6602.9390 | 0.3295   | 0.3470   | Train:0.33s\n",
      "   60    | 6558.7972 | 0.3396   | 0.5841   | Train:0.33s\n",
      "   65    | 12915.1411 | 0.2645   | 0.5664   | Train:0.33s\n",
      "   70    | 1707.5233 | 0.3359   | 0.3419   | Train:0.33s\n",
      "   75    | 13928.5814 | 0.3951   | 0.5797   | Train:0.34s\n",
      "   80    | 5450.0073 | 0.3257   | 0.4433   | Train:0.34s\n",
      "   85    | 1111.1648 | 0.3385   | 0.4623   | Train:0.33s\n",
      "   90    | 930.8967 | 0.3716   | 0.4125   | Train:0.33s\n",
      "   95    | 249.7888 | 0.3610   | 0.2638   | Train:0.33s\n",
      "   100   | 204.1521 | 0.3633   | 0.3276   | Train:0.33s\n",
      "   105   | 144.0312 | 0.3430   | 0.3079   | Train:0.33s\n",
      "   110   | 2673.0644 | 0.2960   | 0.4102   | Train:0.33s\n",
      "   115   | 470.7155 | 0.3638   | 0.4404   | Train:0.33s\n",
      "   120   | 95.5920  | 0.3292   | 0.2802   | Train:0.33s\n",
      "   125   | 62.3764  | 0.3253   | 0.3797   | Train:0.33s\n",
      "   130   | 5.4149   | 0.3452   | 0.4039   | Train:0.33s\n",
      "   135   | 8.8319   | 0.3134   | 0.3173   | Train:0.34s\n",
      "   140   | 4.3999   | 0.2999   | 0.4009   | Train:0.33s\n",
      "   145   | 1.9948   | 0.1666   | 0.3701   | Train:0.34s\n",
      "   150   | 1.4615   | 0.1685   | 0.2983   | Train:0.33s\n",
      "   155   | 1.1214   | 0.1817   | 0.4113   | Train:0.33s\n",
      "   160   | 1.5351   | 0.3229   | 0.5927   | Train:0.33s\n",
      "   165   | 1.1288   | 0.3775   | 0.6044   | Train:0.34s\n",
      "   170   | 0.9705   | 0.4502   | 0.5788   | Train:0.33s\n",
      "   175   | 1.0113   | 0.3623   | 0.4552   | Train:0.33s\n",
      "   180   | 0.8480   | 0.4435   | 0.5788   | Train:0.33s\n",
      "   185   | 0.7816   | 0.4195   | 0.6003   | Train:0.34s\n",
      "   190   | 0.7225   | 0.3811   | 0.6205   | Train:0.34s\n",
      "   195   | 0.6523   | 0.4237   | 0.6040   | Train:0.34s\n",
      "   200   | 0.8116   | 0.3722   | 0.4408   | Train:0.34s\n",
      "   205   | 0.6977   | 0.3710   | 0.4663   | Train:0.34s\n",
      "   210   | 0.5689   | 0.4217   | 0.6239   | Train:0.34s\n",
      "   215   | 0.6142   | 0.3560   | 0.6136   | Train:0.33s\n",
      "   220   | 0.5806   | 0.3965   | 0.4569   | Train:0.34s\n",
      "   225   | 0.5670   | 0.3949   | 0.6070   | Train:0.33s\n",
      "   230   | 0.6468   | 0.3941   | 0.4440   | Train:0.33s\n",
      "   235   | 0.5430   | 0.4224   | 0.4475   | Train:0.33s\n",
      "   240   | 0.5122   | 0.4057   | 0.4433   | Train:0.34s\n",
      "   245   | 0.4881   | 0.4140   | 0.4566   | Train:0.34s\n",
      "   250   | 0.9300   | 0.3966   | 0.4575   | Train:0.34s\n",
      "   255   | 0.4944   | 0.4197   | 0.4555   | Train:0.34s\n",
      "   260   | 0.4715   | 0.4227   | 0.4652   | Train:0.34s\n",
      "   265   | 0.4890   | 0.4146   | 0.4636   | Train:0.34s\n",
      "   270   | 0.4769   | 0.4252   | 0.4631   | Train:0.33s\n",
      "   275   | 2.4399   | 0.4034   | 0.4603   | Train:0.33s\n",
      "   280   | 0.4542   | 0.4046   | 0.4535   | Train:0.33s\n",
      "   285   | 0.4637   | 0.4074   | 0.4723   | Train:0.33s\n",
      "   290   | 0.4639   | 0.4176   | 0.4690   | Train:0.33s\n",
      "   295   | 0.4794   | 0.4381   | 0.4793   | Train:0.34s\n",
      "   300   | 0.4518   | 0.4294   | 0.4801   | Train:0.34s\n",
      "   305   | 0.4404   | 0.4237   | 0.4801   | Train:0.34s\n",
      "   310   | 0.4299   | 0.4317   | 0.4798   | Train:0.34s\n",
      "\n",
      " Early stopping at epoch 310 (patience=20)\n",
      "\n",
      "   FINAL RESULTS:\n",
      "   Train: F1=0.4317, AUC=0.7698, Acc=0.9354, Loss=0.4299\n",
      "   Val:   F1=0.4798, AUC=0.8908, Acc=0.9288\n",
      "   Test:  F1=0.3306, AUC=0.7392, Acc=0.9021\n",
      "      ‚Ä¢ Validation Phase: 13.6s (11.6% of training)\n",
      "      ‚Ä¢ Avg per epoch: Training=0.34s\n",
      "================================================================================\n",
      "ULTRA-OPTIMIZED MODEL TRAINING COMPLETE!\n",
      "Samplers created ONCE for maximum efficiency!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_types = [\n",
    "    \"sampled_gcn\",\n",
    "    \"standard_gcn\",\n",
    "]\n",
    "\n",
    "model_names = {\n",
    "    \"standard_gcn\": \"Standard GCN\",\n",
    "    \"sampled_gcn\": f\"GCN + Hub-Aware Sampling\"\n",
    "}\n",
    "\n",
    "# Store results for each model type and K value\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "all_timings = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if model_type.startswith('sampled'):\n",
    "        print(f\"TRAINING SAMPLING MODEL: {model_names[model_type]}\")\n",
    "    else:\n",
    "        print(f\"TRAINING STANDARD MODEL: {model_names[model_type]}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    all_results[model_type] = {}\n",
    "    all_models[model_type] = {}\n",
    "    all_timings[model_type] = {}\n",
    "    \n",
    "    for K in CONFIG['observation_windows']:\n",
    "        print(f\"\\nModel: {model_names[model_type]} | K={K}\")\n",
    "        \n",
    "        # Start total timing for this configuration\n",
    "        total_start_time = time.time()\n",
    "        \n",
    "        train_graphs = graphs[K]['train']['graphs']\n",
    "        val_graphs = graphs[K]['val']['graphs']\n",
    "        test_graphs = graphs[K]['test']['graphs']\n",
    "        \n",
    "        # Time model initialization\n",
    "        init_start_time = time.time()\n",
    "        num_features = list(train_graphs.values())[0].x.shape[1]\n",
    "        model = create_model(\n",
    "            model_type=model_type,\n",
    "            num_features=num_features,\n",
    "            hidden_dim=CONFIG['hidden_dim'],\n",
    "            num_classes=2,\n",
    "            dropout=CONFIG['dropout'],\n",
    "            normalize=CONFIG['normalize']\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=CONFIG['learning_rate'],\n",
    "            weight_decay=CONFIG['weight_decay']\n",
    "        )\n",
    "        init_time = time.time() - init_start_time\n",
    "        \n",
    "        # Compute class weights\n",
    "        all_train_labels = []\n",
    "        for g in train_graphs.values():\n",
    "            all_train_labels.append(g.y[g.eval_mask].cpu())\n",
    "        all_train_labels = torch.cat(all_train_labels).long()\n",
    "        \n",
    "        class_counts = torch.bincount(all_train_labels)\n",
    "        class_weights = torch.sqrt(1.0 / class_counts.float())\n",
    "        class_weights = class_weights / class_weights.sum() * 2.0\n",
    "        class_weights = class_weights.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # Training loop with comprehensive timing tracking\n",
    "        best_val_f1 = 0\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Universal timing tracking for all models\n",
    "        epoch_times = []\n",
    "        training_times = []  # Time spent on training per epoch\n",
    "        validation_times = []  # Time spent on validation per epoch\n",
    "        train_losses = []  # Track training losses\n",
    "        \n",
    "        # Start training timing\n",
    "        training_start_time = time.time()\n",
    "        \n",
    "        # Check if this is a sampling model\n",
    "        is_sampling_model = model_type.startswith('sampled') and CONFIG['enable_sampling']\n",
    "        \n",
    "        # CREATE HUB-AWARE SAMPLERS ONCE FOR ENTIRE TRAINING (ULTRA-OPTIMIZATION!)\n",
    "        print(f\"   üîß Creating hub-aware adaptive samplers once for entire training...\")\n",
    "        sampler_creation_start = time.time()\n",
    "        train_sampler_data = create_hub_aware_samplers(train_graphs, CONFIG, model_type)\n",
    "        val_sampler_data = create_hub_aware_samplers(val_graphs, CONFIG, model_type)\n",
    "        test_sampler_data = create_hub_aware_samplers(test_graphs, CONFIG, model_type)\n",
    "        sampler_creation_time = time.time() - sampler_creation_start\n",
    "        \n",
    "\n",
    "        print(f\"   {'Epoch':<5} | {'Loss':<8} | {'Train F1':<8} | {'Val F1':<8} | {'Epoch Time':<10} | {'Details'}\")\n",
    "        print(f\"   {'‚îÄ' * 75}\")\n",
    "        \n",
    "        pbar = tqdm(range(CONFIG['epochs']), desc=f\"{model_names[model_type]} K={K}\")\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            # Time individual epoch\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # TRAINING PHASE TIMING\n",
    "            train_start = time.time()\n",
    "            \n",
    "            if is_sampling_model:\n",
    "                # Ultra-optimized hub-aware training using adaptive sampling\n",
    "                train_loss, train_acc = train_epoch_with_hub_aware_samplers(\n",
    "                    model, train_sampler_data, optimizer, criterion, CONFIG, model_type\n",
    "                )\n",
    "            else:\n",
    "                # Standard training for non-sampling strategiesmodels\n",
    "                train_loss, train_acc = train_epoch_with_hub_aware_samplers(\n",
    "                    model, train_sampler_data, optimizer, criterion, CONFIG, model_type\n",
    "                )\n",
    "            \n",
    "            training_time_this_epoch = time.time() - train_start\n",
    "            training_times.append(training_time_this_epoch)\n",
    "            train_losses.append(train_loss)  # Track loss\n",
    "            \n",
    "            # VALIDATION PHASE TIMING (every 5 epochs)\n",
    "            validation_time_this_epoch = 0\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                val_start = time.time()\n",
    "                val_metrics = evaluate_with_hub_aware_samplers(model, val_sampler_data, CONFIG, model_type)\n",
    "                train_metrics = evaluate_with_hub_aware_samplers(model, train_sampler_data, CONFIG, model_type)\n",
    "                validation_time_this_epoch = time.time() - val_start\n",
    "                validation_times.append(validation_time_this_epoch)\n",
    "                \n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                epoch_times.append(epoch_time)\n",
    "                \n",
    "                # Print progress with universal timing breakdown\n",
    "                details = f\"Train:{training_time_this_epoch:.2f}s\"\n",
    "                \n",
    "                print(f\"   {epoch+1:<5} | {train_loss:<8.4f} | {train_metrics['f1']:<8.4f} | {val_metrics['f1']:<8.4f} | {details}\")\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f\"{train_loss:.4f}\",\n",
    "                    'train_f1': f\"{train_metrics['f1']:.4f}\",\n",
    "                    'val_f1': f\"{val_metrics['f1']:.4f}\",\n",
    "                    'epoch_time': f\"{epoch_time:.2f}s\"\n",
    "                })\n",
    "                \n",
    "                if val_metrics['f1'] > best_val_f1:\n",
    "                    best_val_f1 = val_metrics['f1']\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= CONFIG['patience']:\n",
    "                    print(f\"\\n Early stopping at epoch {epoch+1} (patience={CONFIG['patience']})\")\n",
    "                    break\n",
    "        \n",
    "        training_time = time.time() - training_start_time\n",
    "        \n",
    "        # Load best model and evaluate on both validation and test sets\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Time final evaluation using hub-aware samplers\n",
    "        final_eval_start = time.time()\n",
    "        train_metrics = evaluate_with_hub_aware_samplers(model, train_sampler_data, CONFIG, model_type)\n",
    "        val_metrics = evaluate_with_hub_aware_samplers(model, val_sampler_data, CONFIG, model_type)\n",
    "        test_metrics = evaluate_with_hub_aware_samplers(model, test_sampler_data, CONFIG, model_type)\n",
    "        final_eval_time = time.time() - final_eval_start\n",
    "        \n",
    "        total_time = time.time() - total_start_time\n",
    "        \n",
    "        # Store comprehensive timing information with universal train/validation split\n",
    "        timing_info = {\n",
    "            'total_time': total_time,\n",
    "            'init_time': init_time,\n",
    "            'sampler_creation_time': sampler_creation_time,\n",
    "            'total_training_time': training_time,\n",
    "            'final_eval_time': final_eval_time,\n",
    "            'avg_epoch_time': np.mean(epoch_times) if epoch_times else 0,\n",
    "            'total_epochs': len(epoch_times),\n",
    "            'final_loss': train_losses[-1] if train_losses else 0,\n",
    "            # Universal training/validation timing breakdown\n",
    "            'avg_training_time_per_epoch': np.mean(training_times) if training_times else 0,\n",
    "            'total_validation_phase_time': np.sum(validation_times) if validation_times else 0,\n",
    "            'training_percentage': (np.sum(training_times) / training_time * 100) if training_times and training_time > 0 else 0,\n",
    "            'validation_percentage': (np.sum(validation_times) / training_time * 100) if validation_times and training_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        all_timings[model_type][K] = timing_info\n",
    "        \n",
    "        # Enhanced display with loss information and universal timing breakdown\n",
    "        print(f\"\\n   FINAL RESULTS:\")\n",
    "        print(f\"   Train: F1={train_metrics['f1']:.4f}, AUC={train_metrics['auc']:.4f}, Acc={train_metrics['accuracy']:.4f}, Loss={timing_info['final_loss']:.4f}\")\n",
    "        print(f\"   Val:   F1={val_metrics['f1']:.4f}, AUC={val_metrics['auc']:.4f}, Acc={val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Test:  F1={test_metrics['f1']:.4f}, AUC={test_metrics['auc']:.4f}, Acc={test_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Show universal timing breakdown with hub analysis\n",
    "        if is_sampling_model:\n",
    "            print(f\"   üîß Hub-Aware Samplers: {sampler_creation_time:.2f}s (intelligent adaptive sampling!)\")\n",
    "        \n",
    "        # Universal training/validation timing breakdown (applies to all models)\n",
    "        if training_times or validation_times:\n",
    "            if validation_times:\n",
    "                print(f\"      ‚Ä¢ Validation Phase: {timing_info['total_validation_phase_time']:.1f}s ({timing_info['validation_percentage']:.1f}% of training)\")\n",
    "            print(f\"      ‚Ä¢ Avg per epoch: Training={timing_info['avg_training_time_per_epoch']:.2f}s\", end=\"\")\n",
    "        all_results[model_type][K] = {\n",
    "            'train': train_metrics, \n",
    "            'val': val_metrics, \n",
    "            'test': test_metrics,\n",
    "            'timing': timing_info\n",
    "        }\n",
    "        all_models[model_type][K] = model\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ULTRA-OPTIMIZED MODEL TRAINING COMPLETE!\")\n",
    "print(\"Samplers created ONCE for maximum efficiency!\")\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
