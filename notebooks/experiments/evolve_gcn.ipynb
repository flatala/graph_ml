{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal GCN (EvolveGCN-style)\n",
    "\n",
    "**Objective**: Temporal GNN that processes graph sequences over time.\n",
    "\n",
    "**Key principle**: Per-cohort training with state reset. Each cohort C_t gets K+1 graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from code_lib.temporal_node_classification_builder import (\n",
    "    TemporalNodeClassificationBuilder,\n",
    "    load_elliptic_data,\n",
    "    prepare_temporal_model_graphs\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_config import EXPERIMENT_CONFIG\n",
    "\n",
    "CONFIG = EXPERIMENT_CONFIG.copy()\n",
    "# Temporal GNN specific settings\n",
    "CONFIG['epochs'] = 50\n",
    "CONFIG['patience'] = 10\n",
    "\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Observation windows: {CONFIG['observation_windows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df, edges_df = load_elliptic_data(CONFIG['data_dir'], use_temporal_features=True)\n",
    "\n",
    "builder = TemporalNodeClassificationBuilder(\n",
    "    nodes_df=nodes_df,\n",
    "    edges_df=edges_df,\n",
    "    include_class_as_feature=False,\n",
    "    add_temporal_features=True,\n",
    "    cache_dir='../../graph_cache',\n",
    "    use_cache=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "split = builder.get_train_val_test_split(\n",
    "    train_timesteps=CONFIG['train_timesteps'],\n",
    "    val_timesteps=CONFIG['val_timesteps'],\n",
    "    test_timesteps=CONFIG['test_timesteps'],\n",
    "    filter_unknown=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(split['train'])} nodes\")\n",
    "print(f\"Val:   {len(split['val'])} nodes\")\n",
    "print(f\"Test:  {len(split['test'])} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Per-Cohort Temporal Sequences\n",
    "\n",
    "Each cohort C_t gets its own sequence of K+1 graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(CONFIG['device'])\n",
    "\n",
    "sequences = prepare_temporal_model_graphs(\n",
    "    builder,\n",
    "    split['train'],\n",
    "    split['val'],\n",
    "    split['test'],\n",
    "    K_values=CONFIG['observation_windows'],\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal GCN Model with State Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalGCN(nn.Module):\n",
    "    \"\"\"Temporal GCN with LSTM and state reset capability.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, hidden_dim, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GCNConv(num_features, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "        \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset LSTM hidden state between cohorts.\"\"\"\n",
    "        self.h = None\n",
    "        self.c = None\n",
    "    \n",
    "    def forward_one_step(self, x, edge_index):\n",
    "        \"\"\"Process one graph in the sequence.\"\"\"\n",
    "        # GCN layers\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        \n",
    "        # Aggregate to graph-level representation\n",
    "        graph_emb = x.mean(dim=0, keepdim=True).unsqueeze(1)  # [1, 1, hidden_dim]\n",
    "        \n",
    "        # LSTM update\n",
    "        if self.h is None:\n",
    "            output, (self.h, self.c) = self.lstm(graph_emb)\n",
    "        else:\n",
    "            output, (self.h, self.c) = self.lstm(graph_emb, (self.h, self.c))\n",
    "        \n",
    "        # Broadcast LSTM output back to nodes\n",
    "        lstm_out = output.squeeze(1).expand(x.shape[0], -1)  # [num_nodes, hidden_dim]\n",
    "        \n",
    "        # Combine GCN and LSTM features\n",
    "        combined = x + lstm_out\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def classify(self, embeddings):\n",
    "        \"\"\"Classify nodes from final embeddings.\"\"\"\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions (Per-Cohort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_per_cohort(model, cohorts, optimizer, criterion):\n",
    "    \"\"\"Train on all cohorts, resetting state between each.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for cohort in cohorts:\n",
    "        # CRITICAL: Reset state for each cohort\n",
    "        model.reset_state()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed sequence of K+1 graphs\n",
    "        embeddings = None\n",
    "        for graph in cohort['graphs']:\n",
    "            embeddings = model.forward_one_step(graph.x, graph.edge_index)\n",
    "        \n",
    "        # Classify using final embeddings\n",
    "        logits = model.classify(embeddings)\n",
    "        \n",
    "        # Loss only on this cohort's nodes\n",
    "        cohort_indices = cohort['eval_indices']\n",
    "        final_graph = cohort['graphs'][-1]\n",
    "        \n",
    "        loss = criterion(logits[cohort_indices], final_graph.y[cohort_indices])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item() * len(cohort_indices)\n",
    "        pred = logits[cohort_indices].argmax(dim=1)\n",
    "        total_correct += (pred == final_graph.y[cohort_indices]).sum().item()\n",
    "        total_samples += len(cohort_indices)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "def evaluate_per_cohort(model, cohorts):\n",
    "    \"\"\"Evaluate on all cohorts, resetting state between each.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for cohort in cohorts:\n",
    "            # CRITICAL: Reset state for each cohort\n",
    "            model.reset_state()\n",
    "            \n",
    "            # Feed sequence\n",
    "            embeddings = None\n",
    "            for graph in cohort['graphs']:\n",
    "                embeddings = model.forward_one_step(graph.x, graph.edge_index)\n",
    "            \n",
    "            # Classify\n",
    "            logits = model.classify(embeddings)\n",
    "            \n",
    "            # Extract predictions for this cohort\n",
    "            cohort_indices = cohort['eval_indices']\n",
    "            final_graph = cohort['graphs'][-1]\n",
    "            \n",
    "            pred = logits[cohort_indices].argmax(dim=1).cpu().numpy()\n",
    "            true = final_graph.y[cohort_indices].cpu().numpy()\n",
    "            probs = F.softmax(logits[cohort_indices], dim=1)[:, 1].cpu().numpy()\n",
    "            \n",
    "            all_preds.append(pred)\n",
    "            all_labels.append(true)\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    # Concatenate all cohort predictions\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    \n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', pos_label=1, zero_division=0\n",
    "    )\n",
    "    auc = roc_auc_score(all_labels, all_probs) if len(np.unique(all_labels)) > 1 else 0.5\n",
    "    \n",
    "    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models (Per-K Retraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "models = {}\n",
    "\n",
    "for K in CONFIG['observation_windows']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training with K={K}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    train_cohorts = sequences[K]['train']['cohorts']\n",
    "    val_cohorts = sequences[K]['val']['cohorts']\n",
    "    test_cohorts = sequences[K]['test']['cohorts']\n",
    "    \n",
    "    print(f\"Train cohorts: {len(train_cohorts)}\")\n",
    "    print(f\"Val cohorts:   {len(val_cohorts)}\")\n",
    "    print(f\"Test cohorts:  {len(test_cohorts)}\")\n",
    "    print(f\"Graphs per cohort: {K+1}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = train_cohorts[0]['graphs'][0].x.shape[1]\n",
    "    model = TemporalGCN(\n",
    "        num_features=num_features,\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        num_classes=2,\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Compute class weights from all training cohorts\n",
    "    all_train_labels = []\n",
    "    for cohort in train_cohorts:\n",
    "        final_graph = cohort['graphs'][-1]\n",
    "        cohort_labels = final_graph.y[cohort['eval_indices']].cpu()\n",
    "        all_train_labels.append(cohort_labels)\n",
    "    all_train_labels = torch.cat(all_train_labels).long()\n",
    "    \n",
    "    class_counts = torch.bincount(all_train_labels)\n",
    "    class_weights = 1.0 / class_counts.float()\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    class_weights = class_weights.to(device)\n",
    "    \n",
    "    print(f\"Class distribution: Class 0={class_counts[0]:,}, Class 1={class_counts[1]:,}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    pbar = tqdm(range(CONFIG['epochs']), desc=f\"K={K}\")\n",
    "    for epoch in pbar:\n",
    "        train_loss, train_acc = train_epoch_per_cohort(\n",
    "            model, train_cohorts, optimizer, criterion\n",
    "        )\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            val_metrics = evaluate_per_cohort(model, val_cohorts)\n",
    "            pbar.set_postfix({'loss': f\"{train_loss:.4f}\", 'val_f1': f\"{val_metrics['f1']:.4f}\"})\n",
    "            \n",
    "            if val_metrics['f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict().copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    train_metrics = evaluate_per_cohort(model, train_cohorts)\n",
    "    val_metrics = evaluate_per_cohort(model, val_cohorts)\n",
    "    test_metrics = evaluate_per_cohort(model, test_cohorts)\n",
    "    \n",
    "    print(f\"\\nTrain: F1={train_metrics['f1']:.4f}, AUC={train_metrics['auc']:.4f}\")\n",
    "    print(f\"Val:   F1={val_metrics['f1']:.4f}, AUC={val_metrics['auc']:.4f}\")\n",
    "    print(f\"Test:  F1={test_metrics['f1']:.4f}, AUC={test_metrics['auc']:.4f}\")\n",
    "    \n",
    "    results[K] = {'train': train_metrics, 'val': val_metrics, 'test': test_metrics}\n",
    "    models[K] = model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Training complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = []\n",
    "\n",
    "for K in CONFIG['observation_windows']:\n",
    "    metrics = results[K]['test']\n",
    "    comparison_data.append({\n",
    "        'K': K,\n",
    "        'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "        'Precision': f\"{metrics['precision']:.4f}\",\n",
    "        'Recall': f\"{metrics['recall']:.4f}\",\n",
    "        'F1': f\"{metrics['f1']:.4f}\",\n",
    "        'AUC': f\"{metrics['auc']:.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 Score\n",
    "ax = axes[0]\n",
    "f1_scores = [results[K]['test']['f1'] for K in CONFIG['observation_windows']]\n",
    "ax.plot(CONFIG['observation_windows'], f1_scores, marker='o', linewidth=2, color='red')\n",
    "ax.set_xlabel('Observation Window K', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Temporal GCN: F1 Score vs Observation Window', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "ax = axes[1]\n",
    "auc_scores = [results[K]['test']['auc'] for K in CONFIG['observation_windows']]\n",
    "ax.plot(CONFIG['observation_windows'], auc_scores, marker='o', linewidth=2, color='blue')\n",
    "ax.set_xlabel('Observation Window K', fontsize=12)\n",
    "ax.set_ylabel('AUC', fontsize=12)\n",
    "ax.set_title('Temporal GCN: AUC vs Observation Window', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('../../results', exist_ok=True)\n",
    "os.makedirs('../../models', exist_ok=True)\n",
    "\n",
    "comparison_df.to_csv('../../results/temporal_gcn_results.csv', index=False)\n",
    "print(\"Results saved to ../../results/temporal_gcn_results.csv\")\n",
    "\n",
    "for K, model in models.items():\n",
    "    torch.save(model.state_dict(), f'../../models/temporal_gcn_k{K}.pt')\n",
    "print(f\"Models saved to ../../models/temporal_gcn_k*.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
