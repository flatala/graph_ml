{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Model Comparison\n",
    "\n",
    "**Objective**: Compare all models (Temporal GCN, Static GCN, MLP + Graph Features, Baselines) across observation windows K.\n",
    "\n",
    "**Data source**: Pre-computed multi-seed results from all experiment notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define result directories for each model\n",
    "results_dirs = {\n",
    "    'Temporal GCN': Path('../../results/evolve_gcn_multi_seed'),\n",
    "    'Static GCN': Path('../../results/static_gcn_multi_seed'),\n",
    "    'MLP + Graph Features': Path('../../results/graph_features_baseline_multi_seed'),\n",
    "    'Logistic Regression': Path('../../results/baselines_multi_seed/logistic_regression'),\n",
    "    'Random Forest': Path('../../results/baselines_multi_seed/random_forest'),\n",
    "    'XGBoost': Path('../../results/baselines_multi_seed/xgboost')\n",
    "}\n",
    "\n",
    "# Load all results\n",
    "all_models_results = {}\n",
    "\n",
    "for model_name, result_dir in results_dirs.items():\n",
    "    csv_path = result_dir / 'all_seeds_all_metrics.csv'\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['model'] = model_name\n",
    "        all_models_results[model_name] = df\n",
    "        print(f\"✓ Loaded {model_name}: {len(df)} rows\")\n",
    "    else:\n",
    "        print(f\"✗ Missing: {csv_path}\")\n",
    "\n",
    "# Combine all results\n",
    "combined_df = pd.concat(all_models_results.values(), ignore_index=True)\n",
    "print(f\"\\nTotal combined results: {len(combined_df)} rows\")\n",
    "print(f\"Models: {combined_df['model'].unique()}\")\n",
    "print(f\"K values: {sorted(combined_df['K'].unique())}\")\n",
    "print(f\"Splits: {combined_df['split'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter test set only\n",
    "test_df = combined_df[combined_df['split'] == 'test'].copy()\n",
    "\n",
    "# Compute mean and std for each model/K combination\n",
    "summary_stats = test_df.groupby(['model', 'K']).agg({\n",
    "    'f1': ['mean', 'std'],\n",
    "    'auc': ['mean', 'std'],\n",
    "    'precision': ['mean', 'std'],\n",
    "    'recall': ['mean', 'std'],\n",
    "    'accuracy': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "summary_stats.columns = ['_'.join(col).strip('_') for col in summary_stats.columns.values]\n",
    "\n",
    "print(\"Summary statistics computed:\")\n",
    "print(summary_stats.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: F1 Score Comparison (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model_name in results_dirs.keys():\n",
    "    model_data = summary_stats[summary_stats['model'] == model_name]\n",
    "    ax.errorbar(\n",
    "        model_data['K'],\n",
    "        model_data['f1_mean'],\n",
    "        yerr=model_data['f1_std'],\n",
    "        marker='o',\n",
    "        linewidth=2,\n",
    "        capsize=5,\n",
    "        label=model_name\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Observation Window K', fontsize=14)\n",
    "ax.set_ylabel('F1 Score', fontsize=14)\n",
    "ax.set_title('F1 Score vs Observation Window (All Models)', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: AUC Comparison (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for model_name in results_dirs.keys():\n",
    "    model_data = summary_stats[summary_stats['model'] == model_name]\n",
    "    ax.errorbar(\n",
    "        model_data['K'],\n",
    "        model_data['auc_mean'],\n",
    "        yerr=model_data['auc_std'],\n",
    "        marker='o',\n",
    "        linewidth=2,\n",
    "        capsize=5,\n",
    "        label=model_name\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Observation Window K', fontsize=14)\n",
    "ax.set_ylabel('AUC', fontsize=14)\n",
    "ax.set_title('AUC vs Observation Window (All Models)', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Heatmap (F1 Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "f1_pivot = summary_stats.pivot(index='model', columns='K', values='f1_mean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(f1_pivot, annot=True, fmt='.3f', cmap='RdYlGn', ax=ax, cbar_kws={'label': 'F1 Score'})\n",
    "ax.set_title('F1 Score Heatmap: Models vs Observation Window K', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Observation Window K', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Best Model per K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each K\n",
    "best_per_k_f1 = summary_stats.loc[summary_stats.groupby('K')['f1_mean'].idxmax()]\n",
    "best_per_k_auc = summary_stats.loc[summary_stats.groupby('K')['auc_mean'].idxmax()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Best F1\n",
    "ax = axes[0]\n",
    "ax.bar(best_per_k_f1['K'], best_per_k_f1['f1_mean'], color='steelblue', alpha=0.7)\n",
    "ax.errorbar(best_per_k_f1['K'], best_per_k_f1['f1_mean'], \n",
    "            yerr=best_per_k_f1['f1_std'], fmt='none', color='black', capsize=5)\n",
    "for i, (k, model) in enumerate(zip(best_per_k_f1['K'], best_per_k_f1['model'])):\n",
    "    ax.text(k, best_per_k_f1['f1_mean'].iloc[i] + 0.02, model, \n",
    "            ha='center', fontsize=9, rotation=45)\n",
    "ax.set_xlabel('Observation Window K', fontsize=12)\n",
    "ax.set_ylabel('Best F1 Score', fontsize=12)\n",
    "ax.set_title('Best Model by F1 Score per K', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Best AUC\n",
    "ax = axes[1]\n",
    "ax.bar(best_per_k_auc['K'], best_per_k_auc['auc_mean'], color='green', alpha=0.7)\n",
    "ax.errorbar(best_per_k_auc['K'], best_per_k_auc['auc_mean'], \n",
    "            yerr=best_per_k_auc['auc_std'], fmt='none', color='black', capsize=5)\n",
    "for i, (k, model) in enumerate(zip(best_per_k_auc['K'], best_per_k_auc['model'])):\n",
    "    ax.text(k, best_per_k_auc['auc_mean'].iloc[i] + 0.02, model, \n",
    "            ha='center', fontsize=9, rotation=45)\n",
    "ax.set_xlabel('Observation Window K', fontsize=12)\n",
    "ax.set_ylabel('Best AUC', fontsize=12)\n",
    "ax.set_title('Best Model by AUC per K', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 5: Variance Analysis (Model Stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average std across all K values for each model\n",
    "stability = summary_stats.groupby('model').agg({\n",
    "    'f1_std': 'mean',\n",
    "    'auc_std': 'mean'\n",
    "}).sort_values('f1_std')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 Stability\n",
    "ax = axes[0]\n",
    "stability['f1_std'].plot(kind='barh', ax=ax, color='coral')\n",
    "ax.set_xlabel('Average F1 Std Deviation', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Stability (F1 Variance)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# AUC Stability\n",
    "ax = axes[1]\n",
    "stability['auc_std'].plot(kind='barh', ax=ax, color='skyblue')\n",
    "ax.set_xlabel('Average AUC Std Deviation', fontsize=12)\n",
    "ax.set_ylabel('Model', fontsize=12)\n",
    "ax.set_title('Model Stability (AUC Variance)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Stability Ranking (lower std = more stable):\")\n",
    "print(stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formatted comparison table\n",
    "comparison_table = summary_stats.copy()\n",
    "comparison_table['F1'] = comparison_table.apply(\n",
    "    lambda x: f\"{x['f1_mean']:.3f} ± {x['f1_std']:.3f}\", axis=1\n",
    ")\n",
    "comparison_table['AUC'] = comparison_table.apply(\n",
    "    lambda x: f\"{x['auc_mean']:.3f} ± {x['auc_std']:.3f}\", axis=1\n",
    ")\n",
    "comparison_table['Precision'] = comparison_table.apply(\n",
    "    lambda x: f\"{x['precision_mean']:.3f} ± {x['precision_std']:.3f}\", axis=1\n",
    ")\n",
    "comparison_table['Recall'] = comparison_table.apply(\n",
    "    lambda x: f\"{x['recall_mean']:.3f} ± {x['recall_std']:.3f}\", axis=1\n",
    ")\n",
    "\n",
    "display_table = comparison_table[['model', 'K', 'F1', 'AUC', 'Precision', 'Recall']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON (Test Set, Mean ± Std)\")\n",
    "print(\"=\"*80)\n",
    "print(display_table.to_string(index=False))\n",
    "\n",
    "# Best performers\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST PERFORMERS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBest F1 Score per K:\")\n",
    "print(best_per_k_f1[['K', 'model', 'f1_mean', 'f1_std']].to_string(index=False))\n",
    "print(\"\\nBest AUC per K:\")\n",
    "print(best_per_k_auc[['K', 'model', 'auc_mean', 'auc_std']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Combined Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined results\n",
    "output_dir = Path('../../results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save full comparison\n",
    "combined_df.to_csv(output_dir / 'all_models_comparison.csv', index=False)\n",
    "print(f\"Saved: {output_dir / 'all_models_comparison.csv'}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats.to_csv(output_dir / 'comparison_summary_statistics.csv', index=False)\n",
    "print(f\"Saved: {output_dir / 'comparison_summary_statistics.csv'}\")\n",
    "\n",
    "# Save formatted table\n",
    "display_table.to_csv(output_dir / 'comparison_formatted.csv', index=False)\n",
    "print(f\"Saved: {output_dir / 'comparison_formatted.csv'}\")\n",
    "\n",
    "print(\"\\n✅ All comparison results saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
