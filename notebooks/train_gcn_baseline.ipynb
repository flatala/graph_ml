{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0761bd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Data\n",
    "from code_lib.graph_builder import build_emergence_graphs_for_time_range\n",
    "from code_lib.utils import load_parts, save_graphs, load_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3dc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../elliptic_dataset\"\n",
    "WALLETS_FEATURES = \"wallets_features_until_t.csv\"\n",
    "WALLETS_CLASSES = \"wallets_classes.csv\"\n",
    "EDGES_PREFIX = \"AddrTxAddr_edgelist_part_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7afb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(os.path.join(DATA_DIR, WALLETS_FEATURES))\n",
    "node_labels = pd.read_csv(os.path.join(DATA_DIR, WALLETS_CLASSES))\n",
    "edges_with_edge_labels = load_parts(DATA_DIR, EDGES_PREFIX)\n",
    "nodes_with_labels = nodes.merge(node_labels, on='address', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528566a8",
   "metadata": {},
   "source": [
    "### Training a baseline\n",
    "\n",
    "Let's train a simple baseline on a bianry graph with walk length 2 and looking 3 time steps ahead.\n",
    "Let's say we train n time steps 1-37 and evaluate on 40-46 (37 + time horizon, otherwise it's cheating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68f3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique addresses across all time: 822942\n",
      "Total time steps: 49\n",
      "Generating 46 graphs (time steps 1 to 46)...\n",
      "\n",
      "Pre-processing edges by time step...\n",
      "\n",
      "Building graphs...\n",
      "  t=1: nodes= 34853, edges=  66836, labels={np.int64(0): 34853}\n",
      "  t=2: nodes= 59236, edges= 199129, labels={np.int64(0): 59236}\n",
      "  t=3: nodes= 78510, edges= 264124, labels={np.int64(0): 78489, np.int64(1): 21}\n",
      "  t=4: nodes= 98707, edges= 331393, labels={np.int64(0): 98668, np.int64(1): 39}\n",
      "  t=5: nodes=120865, edges= 399829, labels={np.int64(0): 119639, np.int64(1): 1226}\n",
      "  t=6: nodes=131985, edges= 436559, labels={np.int64(0): 130744, np.int64(1): 1241}\n",
      "  t=7: nodes=152051, edges= 492636, labels={np.int64(0): 147918, np.int64(1): 4133}\n",
      "  t=8: nodes=176366, edges= 578493, labels={np.int64(0): 171122, np.int64(1): 5244}\n",
      "  t=9: nodes=194983, edges= 638467, labels={np.int64(0): 190355, np.int64(1): 4628}\n",
      "  t=10: nodes=220639, edges= 701970, labels={np.int64(0): 216981, np.int64(1): 3658}\n",
      "  t=11: nodes=239172, edges= 763390, labels={np.int64(0): 238511, np.int64(1): 661}\n",
      "  t=12: nodes=248071, edges= 789186, labels={np.int64(0): 240509, np.int64(1): 7562}\n",
      "  t=13: nodes=268231, edges= 838562, labels={np.int64(0): 268012, np.int64(1): 219}\n",
      "  t=14: nodes=273241, edges= 849218, labels={np.int64(0): 265822, np.int64(1): 7419}\n",
      "  t=15: nodes=285668, edges= 935328, labels={np.int64(0): 275738, np.int64(1): 9930}\n",
      "  t=16: nodes=293042, edges= 952871, labels={np.int64(0): 284778, np.int64(1): 8264}\n",
      "  t=17: nodes=305048, edges= 980905, labels={np.int64(0): 301974, np.int64(1): 3074}\n",
      "  t=18: nodes=310031, edges= 991937, labels={np.int64(0): 297619, np.int64(1): 12412}\n",
      "  t=19: nodes=319086, edges=1009347, labels={np.int64(0): 284063, np.int64(1): 35023}\n",
      "  t=20: nodes=345524, edges=1084995, labels={np.int64(0): 299479, np.int64(1): 46045}\n",
      "  t=21: nodes=375250, edges=1232114, labels={np.int64(0): 336011, np.int64(1): 39239}\n",
      "  t=22: nodes=396470, edges=1301819, labels={np.int64(0): 355452, np.int64(1): 41018}\n",
      "  t=23: nodes=408092, edges=1326365, labels={np.int64(0): 358022, np.int64(1): 50070}\n",
      "  t=24: nodes=423778, edges=1355964, labels={np.int64(0): 375820, np.int64(1): 47958}\n",
      "  t=25: nodes=435606, edges=1429696, labels={np.int64(0): 417446, np.int64(1): 18160}\n",
      "  t=26: nodes=448519, edges=1456557, labels={np.int64(0): 445538, np.int64(1): 2981}\n",
      "  t=27: nodes=454060, edges=1469020, labels={np.int64(0): 451223, np.int64(1): 2837}\n",
      "  t=28: nodes=458733, edges=1478952, labels={np.int64(0): 454636, np.int64(1): 4097}\n",
      "  t=29: nodes=474120, edges=1510474, labels={np.int64(0): 426732, np.int64(1): 47388}\n",
      "  t=30: nodes=482733, edges=1525398, labels={np.int64(0): 431834, np.int64(1): 50899}\n",
      "  t=31: nodes=495199, edges=1553241, labels={np.int64(0): 425270, np.int64(1): 69929}\n",
      "  t=32: nodes=509931, edges=1596832, labels={np.int64(0): 471743, np.int64(1): 38188}\n",
      "  t=33: nodes=521595, edges=1787166, labels={np.int64(0): 447352, np.int64(1): 74243}\n",
      "  t=34: nodes=530840, edges=1810073, labels={np.int64(0): 474089, np.int64(1): 56751}\n",
      "  t=35: nodes=552376, edges=1860027, labels={np.int64(0): 504824, np.int64(1): 47552}\n",
      "  t=36: nodes=582136, edges=1935978, labels={np.int64(0): 581139, np.int64(1): 997}\n",
      "  t=37: nodes=595940, edges=1966199, labels={np.int64(0): 566895, np.int64(1): 29045}\n",
      "  t=38: nodes=610725, edges=1991574, labels={np.int64(0): 573107, np.int64(1): 37618}\n",
      "  t=39: nodes=620835, edges=2011116, labels={np.int64(0): 567500, np.int64(1): 53335}\n",
      "  t=40: nodes=646229, edges=2084318, labels={np.int64(0): 612770, np.int64(1): 33459}\n",
      "  t=41: nodes=667621, edges=2139238, labels={np.int64(0): 633528, np.int64(1): 34093}\n",
      "  t=42: nodes=697242, edges=2215839, labels={np.int64(0): 668502, np.int64(1): 28740}\n",
      "  t=43: nodes=720718, edges=2315829, labels={np.int64(0): 706572, np.int64(1): 14146}\n",
      "  t=44: nodes=738194, edges=2386988, labels={np.int64(0): 728079, np.int64(1): 10115}\n",
      "  t=45: nodes=761396, edges=2552766, labels={np.int64(0): 760237, np.int64(1): 1159}\n",
      "  t=46: nodes=775403, edges=2613128, labels={np.int64(0): 771924, np.int64(1): 3479}\n",
      "\n",
      "Stored 46 graphs\n"
     ]
    }
   ],
   "source": [
    "graphs = build_emergence_graphs_for_time_range(\n",
    "    edges_with_labels_df=edges_with_edge_labels,\n",
    "    nodes_with_classes_df=nodes_with_labels,\n",
    "    first_time_step=1,\n",
    "    last_time_step=49,\n",
    "    max_walk_length=2,\n",
    "    time_horizon=3,\n",
    "    use_distance_labels=False,\n",
    "    keep_class_labels_as_features=True,\n",
    "    add_staleness_feature=True,\n",
    "    ignore_illict=True,\n",
    "    ignore_previously_transacting_with_illicit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START = 1\n",
    "TRAIN_END = 37\n",
    "EVAL_START = 40\n",
    "EVAL_END = 46\n",
    "\n",
    "train_graphs = [graphs[19], graphs[22], graphs[29], graphs[30]]\n",
    "test_graphs = [graphs[33], graphs[34], graphs[38]] # indexed from 0, so -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c036c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d5c1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197111ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "HIDDEN_DIM = 256\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "num_features = train_graphs[0].x.shape[1]\n",
    "num_classes = 2\n",
    "\n",
    "model = GCN(num_features, HIDDEN_DIM, num_classes).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b35bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, graphs, optimizer, loss_fn=None):\n",
    "    \"\"\"\n",
    "    Train model for one epoch across multiple graphs.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        graphs: List of PyTorch Geometric Data objects\n",
    "        optimizer: PyTorch optimizer\n",
    "        loss_fn: Custom loss function (default: None, uses F.cross_entropy)\n",
    "                 Should accept (logits, labels) and return loss tensor\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss across all nodes\n",
    "        avg_acc: Average accuracy across all nodes\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    \n",
    "    for graph in graphs:\n",
    "        graph = graph.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        \n",
    "        # Use custom loss function if provided, otherwise default to cross_entropy\n",
    "        if loss_fn is not None:\n",
    "            loss = loss_fn(out, graph.y)\n",
    "        else:\n",
    "            loss = F.cross_entropy(out, graph.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred == graph.y).sum().item()\n",
    "        \n",
    "        total_loss += loss.item() * graph.num_nodes\n",
    "        total_correct += correct\n",
    "        total_nodes += graph.num_nodes\n",
    "    \n",
    "    avg_loss = total_loss / total_nodes\n",
    "    avg_acc = total_correct / total_nodes\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efsr9onwu0j",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy_loss(logits, labels, pos_weight=10.0):\n",
    "    \"\"\"\n",
    "    Cross entropy loss with class weighting to handle imbalanced data.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs [num_nodes, num_classes]\n",
    "        labels: Ground truth labels [num_nodes]\n",
    "        pos_weight: Weight multiplier for positive class (default: 10.0)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Weighted cross entropy loss\n",
    "    \"\"\"\n",
    "    weights = torch.ones(logits.size(0), device=logits.device)\n",
    "    weights[labels == 1] = pos_weight\n",
    "    return F.cross_entropy(logits, labels, weight=None, reduction='none').mul(weights).mean()\n",
    "\n",
    "\n",
    "def focal_loss(logits, labels, alpha=0.25, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Focal loss for addressing class imbalance by down-weighting easy examples.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs [num_nodes, num_classes]\n",
    "        labels: Ground truth labels [num_nodes]\n",
    "        alpha: Balancing factor (default: 0.25)\n",
    "        gamma: Focusing parameter (default: 2.0)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Focal loss\n",
    "    \"\"\"\n",
    "    ce_loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "\n",
    "def class_balanced_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    Automatically compute class weights based on frequency and apply weighted CE.\n",
    "    \n",
    "    Args:\n",
    "        logits: Model outputs [num_nodes, num_classes]\n",
    "        labels: Ground truth labels [num_nodes]\n",
    "    \n",
    "    Returns:\n",
    "        loss: Class-balanced cross entropy loss\n",
    "    \"\"\"\n",
    "    # Count class frequencies\n",
    "    unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "    total = labels.size(0)\n",
    "    \n",
    "    # Compute inverse frequency weights\n",
    "    weights = torch.ones(logits.size(1), device=logits.device)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        weights[label] = total / (len(unique_labels) * count)\n",
    "    \n",
    "    return F.cross_entropy(logits, labels, weight=weights)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# loss_fn = lambda logits, labels: weighted_cross_entropy_loss(logits, labels, pos_weight=20.0)\n",
    "# loss_fn = lambda logits, labels: focal_loss(logits, labels, alpha=0.25, gamma=2.0)\n",
    "loss_fn = class_balanced_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c4368c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graphs):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in graphs:\n",
    "            graph = graph.to(DEVICE)\n",
    "            out = model(graph.x, graph.edge_index)\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            preds = out.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(graph.y.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e51cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch   5 | Train Loss: 46029.1513 | Train Acc: 0.6537: 100%|██████████| 5/5 [03:30<00:00, 42.11s/it]                      \n"
     ]
    }
   ],
   "source": [
    "train_loop = tqdm(range(1, NUM_EPOCHS + 1))\n",
    "for epoch in train_loop:\n",
    "    train_loss, train_acc = train_epoch(model, train_graphs, optimizer)\n",
    "\n",
    "    text = f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\"\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        # Evaluate on eval set periodically\n",
    "        eval_preds, eval_labels, eval_probs = evaluate(model, test_graphs)\n",
    "        eval_acc = (eval_preds == eval_labels).mean()\n",
    "        \n",
    "        text = f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Eval Acc: {eval_acc:.4f}\"\n",
    "\n",
    "    train_loop.set_description(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3733f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation Set ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "No Emergence       0.98      0.65      0.78   1546413\n",
      "   Emergence       0.20      0.85      0.32    157638\n",
      "\n",
      "    accuracy                           0.67   1704051\n",
      "   macro avg       0.59      0.75      0.55   1704051\n",
      "weighted avg       0.90      0.67      0.74   1704051\n",
      "\n",
      "ROC-AUC: 0.7309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[999987 546426]\n",
      " [ 24009 133629]]\n",
      "\n",
      "--- Per-Timestep Evaluation ---\n",
      "t=40: Acc=0.6821 | Positives: 50677/56751 | Total: 530840\n",
      "t=41: Acc=0.6769 | Positives: 44149/47552 | Total: 552376\n",
      "t=42: Acc=0.6404 | Positives: 38803/53335 | Total: 620835\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n--- Training Set ---\")\n",
    "# train_preds, train_labels, train_probs = evaluate(model, train_graphs)\n",
    "# print(classification_report(train_labels, train_preds, target_names=['No Emergence', 'Emergence']))\n",
    "# if len(np.unique(train_labels)) == 2:\n",
    "#     auc = roc_auc_score(train_labels, train_probs[:, 1])\n",
    "#     print(f\"ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n--- Evaluation Set ---\")\n",
    "eval_preds, eval_labels, eval_probs = evaluate(model, test_graphs)\n",
    "print(classification_report(eval_labels, eval_preds, target_names=['No Emergence', 'Emergence']))\n",
    "if len(np.unique(eval_labels)) == 2:\n",
    "    auc = roc_auc_score(eval_labels, eval_probs[:, 1])\n",
    "    print(f\"ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(eval_labels, eval_preds))\n",
    "\n",
    "print(\"\\n--- Per-Timestep Evaluation ---\")\n",
    "for i, graph in enumerate(test_graphs):\n",
    "    graph = graph.to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        labels = graph.y.cpu().numpy()\n",
    "        \n",
    "        acc = (preds == labels).mean()\n",
    "        pos_count = (labels == 1).sum()\n",
    "        pos_correct = ((preds == 1) & (labels == 1)).sum()\n",
    "        \n",
    "        print(f\"t={EVAL_START + i}: Acc={acc:.4f} | Positives: {pos_correct}/{pos_count} | Total: {len(labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
